"""Unified data access implementation for next_app.

This module re-implements the subset of legacy UnifiedDataAccess that the
new FastAPI backend actually uses, so that next_app no longer needs to
import the root-level unified_data_access module.

The goal is to preserve behaviour and result structure as much as
possible while keeping the implementation self-contained under next_app.
"""

from __future__ import annotations

from typing import Any, Dict, Optional, List, Tuple
import os
import time as time_module
from datetime import datetime, timedelta
from io import BytesIO
from pathlib import Path
from urllib.parse import urlparse, parse_qs
import re
import zipfile

import pandas as pd
import requests

from .data_source_manager_impl import data_source_manager
from ..infra.network_optimizer import network_optimizer
from ..infra.debug_logger import debug_logger


class UnifiedDataAccess:
    """Unified data access facade used by next_app.

    This class mirrors the public API that the new backend depends on:

    - get_stock_info
    - get_stock_data
    - get_realtime_quotes
    - get_financial_data
    - get_fund_flow_data
    - get_risk_data
    - stock_data_fetcher (for technical indicators)

    Internally it still talks to the same low-level data_source_manager
    and helper fetcher classes as the legacy implementation, but the
    orchestration code lives entirely inside next_app.
    """

    def __init__(self) -> None:
        """Initialise unified data access.

        This follows the original implementation: it exposes a
        StockDataFetcher instance via stock_data_fetcher and prepares
        basic DB / API configuration used by some methods.
        """

        from .stock_data_impl import StockDataFetcher

        self.stock_data_fetcher = StockDataFetcher()

        # Local DB config for minute / weekly tables, shared with ingest
        # scripts. Kept for compatibility even if some methods are not
        # used yet by next_app.
        self._db_cfg: Dict[str, Any] = dict(
            host=os.getenv("TDX_DB_HOST", "localhost"),
            port=int(os.getenv("TDX_DB_PORT", "5432")),
            user=os.getenv("TDX_DB_USER", "postgres"),
            password=os.getenv("TDX_DB_PASSWORD", ""),
            dbname=os.getenv("TDX_DB_NAME", "aistock"),
        )
        self._tdx_api_base = os.getenv("TDX_API_BASE", "http://localhost:8080")

    # ------------------------------------------------------------------
    # åŸºç¡€ä»£ç†ï¼šç›´æ¥èµ°æ•°æ®æºç®¡ç†å™¨
    # ------------------------------------------------------------------

    def get_stock_hist_data(
        self,
        symbol: str,
        start_date: Optional[str] = None,
        end_date: Optional[str] = None,
        adjust: str = "qfq",
    ):
        """Proxy to data_source_manager.get_stock_hist_data.

        This keeps the same signature and behaviour as the legacy
        implementation.
        """

        return data_source_manager.get_stock_hist_data(symbol, start_date, end_date, adjust)

    def get_stock_basic_info(self, symbol: str) -> Dict[str, Any]:
        """Get basic stock info via data_source_manager."""

        return data_source_manager.get_stock_basic_info(symbol)

    # ------------------------------------------------------------------
    # è‚¡ç¥¨ä¿¡æ¯ä¸å†å²æ•°æ®
    # ------------------------------------------------------------------

    def get_stock_info(self, symbol: str, analysis_date: Optional[str] = None) -> Dict[str, Any]:
        """è·å–è‚¡ç¥¨å®Œæ•´ä¿¡æ¯ï¼ˆåŒ…å«åŸºæœ¬ä¿¡æ¯ã€å®æ—¶è¡Œæƒ…ã€ä¼°å€¼æŒ‡æ ‡ç­‰ï¼‰.

        This is a direct adaptation of the legacy implementation with the
        same logging and fallback logic, so that upstream analysis
        behaves consistently.
        """

        debug_logger.info(
            "get_stock_infoå¼€å§‹",
            symbol=symbol,
            analysis_date=analysis_date,
            method="get_stock_info",
        )

        # è·å–åŸºæœ¬ä¿¡æ¯
        info = self.get_stock_basic_info(symbol)
        if not info:
            info = {
                "symbol": symbol,
                "name": "æœªçŸ¥",
                "industry": "æœªçŸ¥",
                "market": "æœªçŸ¥",
            }

        # åˆå§‹åŒ–ä¼°å€¼å’Œè¡Œæƒ…å­—æ®µ
        info.setdefault("current_price", "N/A")
        info.setdefault("change_percent", "N/A")
        info.setdefault("pe_ratio", "N/A")
        info.setdefault("pb_ratio", "N/A")
        info.setdefault("market_cap", "N/A")
        info.setdefault("dividend_yield", "N/A")
        info.setdefault("ps_ratio", "N/A")
        info.setdefault("beta", "N/A")
        info.setdefault("52_week_high", "N/A")
        info.setdefault("52_week_low", "N/A")
        info.setdefault("open_price", "N/A")
        info.setdefault("high_price", "N/A")
        info.setdefault("low_price", "N/A")
        info.setdefault("pre_close", "N/A")
        info.setdefault("volume", "N/A")
        info.setdefault("amount", "N/A")
        info.setdefault("quote_source", "N/A")
        info.setdefault("quote_timestamp", "N/A")

        # ä¼˜å…ˆä½¿ç”¨ Tushare è·å–å®æ—¶è¡Œæƒ…å’Œä¼°å€¼æ•°æ®
        if data_source_manager.tushare_available:
            try:
                debug_logger.debug(
                    "å°è¯•ä»Tushareè·å–å®æ—¶è¡Œæƒ…å’Œä¼°å€¼",
                    symbol=symbol,
                    analysis_date=analysis_date,
                )
                ts_code = data_source_manager._convert_to_ts_code(symbol)

                # æ ¹æ®æ—¥æœŸå’Œæ—¶é—´åˆ¤æ–­ï¼Œè·å–åˆé€‚çš„äº¤æ˜“æ—¥
                trade_date = self._get_appropriate_trade_date(analysis_date=analysis_date)
                debug_logger.debug(
                    "é€‰æ‹©çš„äº¤æ˜“æ—¥",
                    trade_date=trade_date,
                    symbol=symbol,
                    analysis_date=analysis_date,
                )

                try:
                    # è·å– daily_basicï¼ˆåŒ…å«å¸‚ç›ˆç‡ã€å¸‚å‡€ç‡ã€å¸‚å€¼ç­‰ï¼‰
                    with network_optimizer.apply():
                        daily_basic = data_source_manager.tushare_api.daily_basic(
                            ts_code=ts_code,
                            trade_date=trade_date,
                        )

                    if daily_basic is not None and not daily_basic.empty:
                        row = daily_basic.iloc[0]

                        # å¸‚ç›ˆç‡ã€å¸‚å‡€ç‡ã€å¸‚å€¼
                        if row.get("pe") and pd.notna(row.get("pe")) and row.get("pe") > 0:
                            info["pe_ratio"] = round(float(row["pe"]), 2)
                        if row.get("pb") and pd.notna(row.get("pb")) and row.get("pb") > 0:
                            info["pb_ratio"] = round(float(row["pb"]), 2)
                        if row.get("total_mv") and pd.notna(row.get("total_mv")):
                            # Tushare å•ä½ï¼šä¸‡å…ƒï¼Œè½¬æ¢ä¸ºå…ƒ
                            info["market_cap"] = float(row["total_mv"]) * 10000

                        debug_logger.debug(
                            "Tushareè·å–daily_basicæˆåŠŸ",
                            symbol=symbol,
                            trade_date=trade_date,
                            pe=info.get("pe_ratio"),
                            pb=info.get("pb_ratio"),
                        )

                        # è·å– daily æ•°æ®ï¼ˆå½“å‰ä»·æ ¼ã€æ¶¨è·Œå¹…ï¼‰
                        with network_optimizer.apply():
                            daily = data_source_manager.tushare_api.daily(
                                ts_code=ts_code,
                                start_date=trade_date,
                                end_date=trade_date,
                            )

                        if daily is not None and not daily.empty:
                            daily_row = daily.iloc[0]
                            info["current_price"] = round(float(daily_row["close"]), 2)
                            info["change_percent"] = round(float(daily_row["pct_chg"]), 2)

                            debug_logger.debug(
                                "Tushareè·å–dailyæˆåŠŸ",
                                symbol=symbol,
                                trade_date=trade_date,
                                price=info.get("current_price"),
                                change_pct=info.get("change_percent"),
                            )
                        else:
                            # å¦‚æœå½“æ—¥æ•°æ®ä¸å¯ç”¨ï¼Œå°è¯•å›é€€åˆ°æœ€è¿‘å‡ ä¸ªäº¤æ˜“æ—¥
                            debug_logger.debug(
                                "å½“æ—¥æ•°æ®ä¸å¯ç”¨ï¼Œå°è¯•å›é€€æŸ¥æ‰¾",
                                trade_date=trade_date,
                            )
                            for days_back in range(1, 5):
                                fallback_date = (
                                    datetime.now() - timedelta(days=days_back)
                                ).strftime("%Y%m%d")
                                try:
                                    with network_optimizer.apply():
                                        daily = data_source_manager.tushare_api.daily(
                                            ts_code=ts_code,
                                            start_date=fallback_date,
                                            end_date=fallback_date,
                                        )
                                    if daily is not None and not daily.empty:
                                        daily_row = daily.iloc[0]
                                        info["current_price"] = round(
                                            float(daily_row["close"]), 2
                                        )
                                        info["change_percent"] = round(
                                            float(daily_row["pct_chg"]), 2
                                        )
                                        debug_logger.debug(
                                            "å›é€€è·å–æ•°æ®æˆåŠŸ",
                                            symbol=symbol,
                                            fallback_date=fallback_date,
                                            price=info.get("current_price"),
                                        )
                                        break
                                except Exception as e:  # noqa: BLE001
                                    debug_logger.debug(
                                        f"å›é€€è·å–{fallback_date}æ•°æ®å¤±è´¥",
                                        error=str(e),
                                    )
                                    continue

                except Exception as e:  # noqa: BLE001
                    debug_logger.warning(
                        f"Tushareè·å–{trade_date}æ•°æ®å¤±è´¥ï¼Œå°è¯•å›é€€",
                        error=str(e),
                        symbol=symbol,
                    )
                    # å¦‚æœé€‰æ‹©çš„äº¤æ˜“æ—¥æ•°æ®è·å–å¤±è´¥ï¼Œå›é€€åˆ°æœ€è¿‘å‡ ä¸ªäº¤æ˜“æ—¥
                    for days_back in range(1, 5):
                        fallback_date = (
                            datetime.now() - timedelta(days=days_back)
                        ).strftime("%Y%m%d")
                        try:
                            with network_optimizer.apply():
                                daily_basic = data_source_manager.tushare_api.daily_basic(
                                    ts_code=ts_code,
                                    trade_date=fallback_date,
                                )
                            if daily_basic is not None and not daily_basic.empty:
                                row = daily_basic.iloc[0]
                                if (
                                    row.get("pe")
                                    and pd.notna(row.get("pe"))
                                    and row.get("pe") > 0
                                ):
                                    info["pe_ratio"] = round(float(row["pe"]), 2)
                                if (
                                    row.get("pb")
                                    and pd.notna(row.get("pb"))
                                    and row.get("pb") > 0
                                ):
                                    info["pb_ratio"] = round(float(row["pb"]), 2)
                                if row.get("total_mv") and pd.notna(row.get("total_mv")):
                                    info["market_cap"] = float(row["total_mv"]) * 10000

                                daily = data_source_manager.tushare_api.daily(
                                    ts_code=ts_code,
                                    start_date=fallback_date,
                                    end_date=fallback_date,
                                )
                                if daily is not None and not daily.empty:
                                    daily_row = daily.iloc[0]
                                    info["current_price"] = round(
                                        float(daily_row["close"]), 2
                                    )
                                    info["change_percent"] = round(
                                        float(daily_row["pct_chg"]), 2
                                    )
                                debug_logger.debug(
                                    "å›é€€è·å–æˆåŠŸ",
                                    fallback_date=fallback_date,
                                    symbol=symbol,
                                )
                                break
                        except Exception as e2:  # noqa: BLE001
                            debug_logger.debug(
                                f"å›é€€è·å–{fallback_date}å¤±è´¥", error=str(e2)
                            )
                            continue

            except Exception as e:  # noqa: BLE001
                debug_logger.warning(
                    "Tushareè·å–å®æ—¶æ•°æ®å¤±è´¥", error=e, symbol=symbol
                )

        # Tushare å¤±è´¥æˆ–æ•°æ®ä¸å®Œæ•´ï¼Œä½¿ç”¨ Akshare å¤‡ç”¨ï¼ˆä»…å®æ—¶æ¨¡å¼ï¼Œå†å²æ¨¡å¼ä¸ä½¿ç”¨ Akshareï¼‰
        if (info["current_price"] == "N/A" or info["pe_ratio"] == "N/A") and not analysis_date:
            try:
                debug_logger.debug("å°è¯•ä»Akshareè·å–è¯¦ç»†ä¿¡æ¯", symbol=symbol)
                with network_optimizer.apply():
                    import akshare as ak  # type: ignore

                    stock_info_df = ak.stock_individual_info_em(symbol=symbol)

                if stock_info_df is not None and not stock_info_df.empty:
                    for _, row in stock_info_df.iterrows():
                        key = row["item"]
                        value = row["value"]

                        if key == "è‚¡ç¥¨ç®€ç§°" and info["name"] == "æœªçŸ¥":
                            info["name"] = value
                        elif key == "æ€»å¸‚å€¼":
                            try:
                                if value and value != "-":
                                    info["market_cap"] = float(value)
                            except Exception:  # noqa: BLE001
                                pass
                        elif key == "å¸‚ç›ˆç‡-åŠ¨æ€" and info["pe_ratio"] == "N/A":
                            try:
                                if value and value != "-":
                                    pe_val = float(value)
                                    if 0 < pe_val <= 1000:
                                        info["pe_ratio"] = pe_val
                            except Exception:  # noqa: BLE001
                                pass
                        elif key == "å¸‚å‡€ç‡" and info["pb_ratio"] == "N/A":
                            try:
                                if value and value != "-":
                                    pb_val = float(value)
                                    if 0 < pb_val <= 100:
                                        info["pb_ratio"] = pb_val
                            except Exception:  # noqa: BLE001
                                pass

                    debug_logger.debug("Akshareè·å–è¯¦ç»†ä¿¡æ¯æˆåŠŸ", symbol=symbol)
            except Exception as e:  # noqa: BLE001
                debug_logger.warning("Akshareè·å–è¯¦ç»†ä¿¡æ¯å¤±è´¥", error=e, symbol=symbol)

        # å®æ—¶æ¨¡å¼ä¸‹ä¼˜å…ˆä½¿ç”¨å®æ—¶è¡Œæƒ…åˆ·æ–°ä»·æ ¼/æ¶¨è·Œå¹…ç­‰å­—æ®µ
        if not analysis_date:
            try:
                debug_logger.debug("å°è¯•ä»å®æ—¶è¡Œæƒ…è·å–ä»·æ ¼", symbol=symbol)
                quotes = self.get_realtime_quotes(symbol)
                if quotes and isinstance(quotes, dict):
                    price_val = quotes.get("price")
                    if price_val is not None:
                        info["current_price"] = round(float(price_val), 2)
                    change_pct_val = quotes.get("change_percent")
                    if change_pct_val is not None:
                        info["change_percent"] = round(float(change_pct_val), 2)
                    open_val = quotes.get("open")
                    if open_val is not None:
                        info["open_price"] = round(float(open_val), 2)
                    high_val = quotes.get("high")
                    if high_val is not None:
                        info["high_price"] = round(float(high_val), 2)
                    low_val = quotes.get("low")
                    if low_val is not None:
                        info["low_price"] = round(float(low_val), 2)
                    pre_close_val = quotes.get("pre_close")
                    if pre_close_val is not None:
                        info["pre_close"] = round(float(pre_close_val), 2)
                    volume_val = quotes.get("volume")
                    if volume_val is not None:
                        try:
                            info["volume"] = int(volume_val)
                        except (TypeError, ValueError):
                            info["volume"] = volume_val
                    amount_val = quotes.get("amount")
                    if amount_val is not None:
                        info["amount"] = round(float(amount_val), 2)
                    if quotes.get("source"):
                        info["quote_source"] = quotes["source"]
                    if quotes.get("timestamp"):
                        info["quote_timestamp"] = quotes["timestamp"]
                    debug_logger.debug(
                        "å®æ—¶è¡Œæƒ…è·å–æˆåŠŸ",
                        symbol=symbol,
                        source=quotes.get("source"),
                    )
            except Exception as e:  # noqa: BLE001
                debug_logger.debug("å®æ—¶è¡Œæƒ…è·å–å¤±è´¥", error=e, symbol=symbol)

        # å¦‚æœè¿˜æ˜¯æ²¡æœ‰ï¼Œå°è¯•ä»å†å²æ•°æ®è·å–æœ€æ–°æ”¶ç›˜ä»·
        if info["current_price"] == "N/A":
            try:
                debug_logger.debug(
                    "å°è¯•ä»å†å²æ•°æ®è·å–æœ€æ–°ä»·æ ¼",
                    symbol=symbol,
                    analysis_date=analysis_date,
                )
                # å¦‚æœæä¾›äº† analysis_dateï¼Œä½¿ç”¨å®ƒä½œä¸ºç»“æŸæ—¥æœŸï¼›å¦åˆ™ä½¿ç”¨å½“å‰æ—¥æœŸ
                if analysis_date:
                    end_date = analysis_date
                    base_date = datetime.strptime(analysis_date, "%Y%m%d")
                else:
                    end_date = datetime.now().strftime("%Y%m%d")
                    base_date = datetime.now()

                start_date = (base_date - timedelta(days=30)).strftime("%Y%m%d")

                hist_data = self.get_stock_hist_data(
                    symbol=symbol,
                    start_date=start_date,
                    end_date=end_date,
                )

                if (
                    hist_data is not None
                    and not hist_data.empty
                    and isinstance(hist_data, pd.DataFrame)
                ):
                    if "close" in hist_data.columns:
                        info["current_price"] = round(
                            float(hist_data.iloc[-1]["close"]), 2
                        )
                        # è®¡ç®—æ¶¨è·Œå¹…
                        if len(hist_data) > 1:
                            prev_close = hist_data.iloc[-2]["close"]
                            change_pct = (
                                (hist_data.iloc[-1]["close"] - prev_close)
                                / prev_close
                            ) * 100
                            info["change_percent"] = round(change_pct, 2)
                        debug_logger.debug("å†å²æ•°æ®è·å–æˆåŠŸ", symbol=symbol)
            except Exception as e:  # noqa: BLE001
                debug_logger.debug("å†å²æ•°æ®è·å–å¤±è´¥", error=e, symbol=symbol)

        # è·å– Beta ç³»æ•°ï¼ˆä»… A è‚¡ï¼Œåœ¨è·å–å®ŒåŸºæœ¬ä¿¡æ¯åï¼‰
        if info.get("beta") == "N/A" and self._is_chinese_stock(symbol):
            try:
                debug_logger.debug("å°è¯•è·å–Betaç³»æ•°", symbol=symbol)
                beta = self.get_beta_coefficient(symbol)
                if beta is not None:
                    info["beta"] = round(float(beta), 4)
                    debug_logger.debug(
                        "Betaç³»æ•°è·å–æˆåŠŸ", symbol=symbol, beta=info["beta"]
                    )
            except Exception as e:  # noqa: BLE001
                debug_logger.debug("Betaç³»æ•°è·å–å¤±è´¥", error=e, symbol=symbol)

        # è·å– 52 å‘¨é«˜ä½ä½ï¼ˆä»… A è‚¡ï¼Œåœ¨è·å–å®ŒåŸºæœ¬ä¿¡æ¯åï¼‰
        if (
            info.get("52_week_high") == "N/A"
            or info.get("52_week_low") == "N/A"
        ) and self._is_chinese_stock(symbol):
            try:
                debug_logger.debug("å°è¯•è·å–52å‘¨é«˜ä½ä½", symbol=symbol)
                week52_data = self.get_52week_high_low(symbol)
                if week52_data and week52_data.get("success"):
                    info["52_week_high"] = week52_data.get("high_52w", "N/A")
                    info["52_week_low"] = week52_data.get("low_52w", "N/A")
                    debug_logger.debug(
                        "52å‘¨é«˜ä½ä½è·å–æˆåŠŸ",
                        symbol=symbol,
                        high=info.get("52_week_high"),
                        low=info.get("52_week_low"),
                    )
            except Exception as e:  # noqa: BLE001
                debug_logger.debug("52å‘¨é«˜ä½ä½è·å–å¤±è´¥", error=e, symbol=symbol)

        debug_logger.info(
            "get_stock_infoå®Œæˆ",
            symbol=symbol,
            has_price=(info.get("current_price") != "N/A"),
            has_pe=(info.get("pe_ratio") != "N/A"),
            has_pb=(info.get("pb_ratio") != "N/A"),
            has_beta=(info.get("beta") != "N/A"),
            has_52week=(info.get("52_week_high") != "N/A"),
        )

        return info

    def get_stock_data(
        self,
        symbol: str,
        period: str = "1y",
        analysis_date: Optional[str] = None,
    ):
        """è·å–è‚¡ç¥¨å†å²æ•°æ®ï¼ˆåˆ«åæ–¹æ³•ï¼Œå…¼å®¹æ—§æ¥å£ï¼‰.

        The logic is kept identical to the legacy implementation so that
        callers relying on date range and normalisation behave the same.
        """

        debug_logger.info(
            "UnifiedDataAccess.get_stock_dataè°ƒç”¨",
            symbol=symbol,
            period=period,
            analysis_date=analysis_date,
            method="get_stock_data",
        )

        # æ ¹æ® period è®¡ç®—æ—¥æœŸèŒƒå›´
        # å¦‚æœæä¾›äº† analysis_dateï¼Œä½¿ç”¨å®ƒä½œä¸ºæˆªæ­¢æ—¥æœŸï¼›å¦åˆ™ä½¿ç”¨å½“å‰æ—¥æœŸ
        if analysis_date:
            end_date = analysis_date  # å·²ç»æ˜¯ 'YYYYMMDD' æ ¼å¼
            base_date = datetime.strptime(analysis_date, "%Y%m%d")
        else:
            end_date = datetime.now().strftime("%Y%m%d")
            base_date = datetime.now()

        period_map = {
            "1mo": 30,
            "3mo": 90,
            "6mo": 180,
            "1y": 365,
            "2y": 730,
            "5y": 1825,
            "max": 3650,
        }
        days = period_map.get(period, 365)
        start_date = (base_date - timedelta(days=days)).strftime("%Y%m%d")

        debug_logger.debug(
            "è®¡ç®—æ—¥æœŸèŒƒå›´",
            start_date=start_date,
            end_date=end_date,
            days=days,
        )

        result = self.get_stock_hist_data(symbol, start_date, end_date)

        debug_logger.data_info("get_stock_hist_dataè¿”å›", result)

        # å¤„ç†è¿”å›ç»“æœ
        if result is None:
            debug_logger.warning(
                "get_stock_hist_dataè¿”å›None", symbol=symbol, period=period
            )
            return None

        # å¦‚æœæ˜¯å­—å…¸ï¼Œå°è¯•è½¬æ¢ä¸º DataFrame æˆ–è¿”å›é”™è¯¯
        if isinstance(result, dict):
            # æ£€æŸ¥æ˜¯å¦æ˜¯é”™è¯¯å“åº”
            if "error" in result:
                debug_logger.error(
                    "æ•°æ®æºè¿”å›é”™è¯¯",
                    error=result.get("error"),
                    symbol=symbol,
                    period=period,
                )
                return None

            # å°è¯•å°†å­—å…¸è½¬æ¢ä¸º DataFrame
            try:
                debug_logger.warning(
                    "å°è¯•å°†dictè½¬æ¢ä¸ºDataFrame",
                    symbol=symbol,
                    dict_keys=list(result.keys()),
                )
                # å¦‚æœæ˜¯å•è¡Œæ•°æ®å­—å…¸ï¼Œè½¬æ¢ä¸º DataFrame
                if all(
                    not isinstance(v, (list, pd.Series)) for v in result.values()
                ):
                    df = pd.DataFrame([result])
                    debug_logger.info(
                        "æˆåŠŸå°†å•è¡Œdictè½¬æ¢ä¸ºDataFrame", symbol=symbol, rows=1
                    )
                    return df
                # å¤šè¡Œæ•°æ®å­—å…¸ï¼Œå°è¯•ç›´æ¥è½¬æ¢
                df = pd.DataFrame(result)
                debug_logger.info(
                    "æˆåŠŸå°†å¤šè¡Œdictè½¬æ¢ä¸ºDataFrame", symbol=symbol, rows=len(df)
                )
                return df
            except Exception as e:  # noqa: BLE001
                debug_logger.error(
                    "æ— æ³•å°†dictè½¬æ¢ä¸ºDataFrame",
                    error=e,
                    symbol=symbol,
                    dict_keys=list(result.keys())[:5],
                )
                return None

        # éªŒè¯è¿”å›ç±»å‹ - å¿…é¡»æ˜¯ DataFrame
        if not isinstance(result, pd.DataFrame):
            debug_logger.error(
                "get_stock_hist_dataè¿”å›ç±»å‹é”™è¯¯",
                expected_type="DataFrame or None",
                actual_type=type(result).__name__,
                symbol=symbol,
                period=period,
                result_preview=str(result)[:200],
            )
            return None

        # æ•°æ®æ ‡å‡†åŒ–ï¼šç¡®ä¿åˆ—åæ­£ç¡®
        try:
            # æ ‡å‡†åŒ–åˆ—åï¼ˆç»Ÿä¸€ä¸ºå¤§å†™ï¼‰
            column_mapping = {
                "date": "Date",
                "open": "Open",
                "high": "High",
                "low": "Low",
                "close": "Close",
                "volume": "Volume",
                "amount": "Amount",
            }

            # é‡å‘½ååˆ—
            result = result.rename(columns=column_mapping)

            # ç¡®ä¿ Date åˆ—ä¸º datetime ç±»å‹å¹¶è®¾ç½®ä¸ºç´¢å¼•
            if "Date" in result.columns:
                result["Date"] = pd.to_datetime(result["Date"])
                result = result.set_index("Date")
            elif result.index.name == "date" or (
                hasattr(result.index, "dtype")
                and "datetime" in str(result.index.dtype)
            ):
                # ç´¢å¼•å·²ç»æ˜¯æ—¥æœŸç±»å‹
                result.index.name = "Date"

            # ç¡®ä¿æ•°å€¼åˆ—ä¸º float ç±»å‹
            numeric_columns = ["Open", "High", "Low", "Close", "Volume"]
            for col in numeric_columns:
                if col in result.columns:
                    result[col] = pd.to_numeric(result[col], errors="coerce")

            # æŒ‰æ—¥æœŸæ’åº
            result = result.sort_index()

            debug_logger.debug(
                "æ•°æ®æ ‡å‡†åŒ–å®Œæˆ",
                symbol=symbol,
                rows=len(result),
                columns=list(result.columns),
                date_range=f"{result.index.min()} ~ {result.index.max()}",
            )

        except Exception as e:  # noqa: BLE001
            debug_logger.error(
                "æ•°æ®æ ‡å‡†åŒ–å¤±è´¥",
                error=e,
                symbol=symbol,
                columns=list(result.columns)
                if hasattr(result, "columns")
                else "N/A",
            )
            # å³ä½¿æ ‡å‡†åŒ–å¤±è´¥ï¼Œä¹Ÿè¿”å›åŸå§‹æ•°æ®

        return result

    # ------------------------------------------------------------------
    # å…¶å®ƒæ•°æ®æ¥å£ï¼šè´¢åŠ¡ã€èµ„é‡‘æµã€é£é™©ã€æƒ…ç»ªã€æ–°é—»ç­‰
    # ------------------------------------------------------------------

    def get_realtime_quotes(self, symbol: str) -> Dict[str, Any]:
        """å®æ—¶è¡Œæƒ…ç›´æ¥ä»£ç† data_source_manager."""

        return data_source_manager.get_realtime_quotes(symbol)

    def get_financial_data(
        self,
        symbol: str,
        report_type: str = "income",
        analysis_date: Optional[str] = None,
    ) -> Dict[str, Any]:
        """è·å–è´¢åŠ¡æ•°æ®ï¼ˆåŒ…è£…ä¸ºå­—å…¸æ ¼å¼ï¼‰ã€‚"""

        debug_logger.info(
            "å¼€å§‹è·å–è´¢åŠ¡æ•°æ®",
            symbol=symbol,
            report_type=report_type,
            analysis_date=analysis_date,
            method="get_financial_data",
        )

        result: Dict[str, Any] = {
            "symbol": symbol,
            "data_success": False,
            "income_statement": None,
            "balance_sheet": None,
            "cash_flow": None,
            "source": None,
        }

        try:
            # æ³¨æ„ï¼šdata_source_manager.get_financial_data() ç›®å‰ä¸æ”¯æŒ analysis_date å‚æ•°
            df = data_source_manager.get_financial_data(symbol, report_type)

            if df is not None and isinstance(df, pd.DataFrame) and not df.empty:
                records = df.to_dict("records")

                if report_type == "income":
                    result["income_statement"] = {
                        "data": records,
                        "periods": len(records),
                        "columns": df.columns.tolist(),
                    }
                elif report_type == "balance":
                    result["balance_sheet"] = {
                        "data": records,
                        "periods": len(records),
                        "columns": df.columns.tolist(),
                    }
                elif report_type == "cashflow":
                    result["cash_flow"] = {
                        "data": records,
                        "periods": len(records),
                        "columns": df.columns.tolist(),
                    }

                result["source"] = (
                    "tushare" if data_source_manager.tushare_available else "akshare"
                )
                result["data_success"] = True

                debug_logger.info(
                    "è´¢åŠ¡æ•°æ®è·å–æˆåŠŸ",
                    symbol=symbol,
                    report_type=report_type,
                    periods=len(records),
                    source=result["source"],
                )
            else:
                result["error"] = f"æœªèƒ½è·å–{report_type}è´¢åŠ¡æ•°æ®"
                debug_logger.warning("è´¢åŠ¡æ•°æ®ä¸ºç©º", symbol=symbol, report_type=report_type)

        except Exception as e:  # noqa: BLE001
            result["error"] = str(e)
            debug_logger.error(
                "è·å–è´¢åŠ¡æ•°æ®å¤±è´¥", error=e, symbol=symbol, report_type=report_type
            )

        return result

    def get_quarterly_reports(
        self, symbol: str, analysis_date: Optional[str] = None
    ) -> Optional[Dict[str, Any]]:
        """è·å–å­£åº¦æŠ¥è¡¨æ•°æ®ï¼ˆç»Ÿä¸€å°è£…ï¼‰ã€‚"""

        try:
            from .quarterly_report_data_impl import QuarterlyReportDataFetcher

            with network_optimizer.apply():
                return QuarterlyReportDataFetcher().get_quarterly_reports(
                    symbol, analysis_date=analysis_date
                )
        except Exception as e:  # noqa: BLE001
            return {"symbol": symbol, "data_success": False, "error": str(e)}

    def get_fund_flow_data(
        self, symbol: str, analysis_date: Optional[str] = None
    ) -> Optional[Dict[str, Any]]:
        """è·å–èµ„é‡‘æµå‘æ•°æ®ï¼ˆAkshare å®ç°ï¼‰ã€‚"""

        try:
            from .fund_flow_akshare_impl import FundFlowAkshareDataFetcher

            with network_optimizer.apply():
                return FundFlowAkshareDataFetcher().get_fund_flow_data(
                    symbol, analysis_date=analysis_date
                )
        except Exception as e:  # noqa: BLE001
            debug_logger.error(
                "è·å–èµ„é‡‘æµå‘æ•°æ®å¤±è´¥",
                symbol=symbol,
                error=str(e),
                analysis_date=analysis_date,
            )
            return {"symbol": symbol, "data_success": False, "error": str(e)}

    def get_market_sentiment_data(
        self,
        symbol: str,
        stock_data,
        analysis_date: Optional[str] = None,
    ) -> Optional[Dict[str, Any]]:
        """è·å–å¸‚åœºæƒ…ç»ªæ•°æ®ã€‚"""

        try:
            from .market_sentiment_data_impl import MarketSentimentDataFetcher

            with network_optimizer.apply():
                return MarketSentimentDataFetcher().get_market_sentiment_data(
                    symbol, stock_data, analysis_date=analysis_date
                )
        except Exception as e:  # noqa: BLE001
            debug_logger.error(
                "è·å–å¸‚åœºæƒ…ç»ªæ•°æ®å¤±è´¥",
                symbol=symbol,
                error=str(e),
                analysis_date=analysis_date,
            )
            return {"symbol": symbol, "data_success": False, "error": str(e)}

    def get_margin_trading_history(
        self,
        symbol: str,
        days: int = 5,
        analysis_date: Optional[str] = None,
    ) -> Optional[Dict[str, Any]]:
        """è·å–ä¸ªè‚¡èèµ„èåˆ¸å†å²æ•°æ®ã€‚"""

        try:
            from .market_sentiment_data_impl import MarketSentimentDataFetcher

            with network_optimizer.apply():
                return MarketSentimentDataFetcher()._get_margin_trading_history(
                    symbol, days=days, analysis_date=analysis_date
                )
        except Exception as e:  # noqa: BLE001
            debug_logger.error(
                "è·å–èèµ„èåˆ¸å†å²æ•°æ®å¤±è´¥",
                symbol=symbol,
                error=str(e),
                analysis_date=analysis_date,
            )
            return {"symbol": symbol, "data_success": False, "error": str(e)}

    def get_index_daily_metrics(
        self, analysis_date: Optional[str] = None
    ) -> Optional[Dict[str, Any]]:
        """è·å–é‡ç‚¹æŒ‡æ•°æ¯æ—¥æŒ‡æ ‡æ•°æ®ã€‚"""

        try:
            from .market_sentiment_data_impl import MarketSentimentDataFetcher

            with network_optimizer.apply():
                return MarketSentimentDataFetcher()._get_index_daily_metrics(
                    analysis_date=analysis_date
                )
        except Exception as e:  # noqa: BLE001
            debug_logger.error(
                "è·å–æŒ‡æ•°æ¯æ—¥æŒ‡æ ‡å¤±è´¥", error=str(e), analysis_date=analysis_date
            )
            return {"data_success": False, "error": str(e)}

    def get_news_data(
        self, symbol: str, analysis_date: Optional[str] = None
    ) -> Optional[Dict[str, Any]]:
        """è·å–æ–°é—»æ•°æ®ã€‚"""

        try:
            from .qstock_news_data_impl import QStockNewsDataFetcher

            with network_optimizer.apply():
                return QStockNewsDataFetcher().get_stock_news(
                    symbol, analysis_date=analysis_date
                )
        except Exception as e:  # noqa: BLE001
            debug_logger.error(
                "è·å–æ–°é—»æ•°æ®å¤±è´¥",
                symbol=symbol,
                error=str(e),
                analysis_date=analysis_date,
            )
            return {"symbol": symbol, "data_success": False, "error": str(e)}

    def get_stock_news(
        self, symbol: str, analysis_date: Optional[str] = None
    ) -> Optional[Dict[str, Any]]:
        """è·å–è‚¡ç¥¨æ–°é—»ï¼ˆåˆ«åæ–¹æ³•ï¼Œå…¼å®¹æ—§æ¥å£ï¼‰ã€‚"""

        return self.get_news_data(symbol, analysis_date=analysis_date)

    def get_risk_data(
        self, symbol: str, analysis_date: Optional[str] = None
    ) -> Optional[Dict[str, Any]]:
        """è·å–é£é™©æ•°æ®ï¼ˆé™å”®è§£ç¦ã€å¤§è‚¡ä¸œå‡æŒç­‰ï¼‰ã€‚"""

        try:
            from .risk_data_fetcher_impl import RiskDataFetcher

            with network_optimizer.apply():
                return RiskDataFetcher().get_risk_data(
                    symbol, analysis_date=analysis_date
                )
        except Exception as e:  # noqa: BLE001
            return {"symbol": symbol, "data_success": False, "error": str(e)}

    def get_research_reports_data(
        self,
        symbol: str,
        days: int = 180,
        analysis_date: Optional[str] = None,
    ) -> Dict[str, Any]:
        """è·å–æœºæ„ç ”æŠ¥æ•°æ® (Tushare ä¼˜å…ˆï¼ŒåŒ…å«ç ”æŠ¥å†…å®¹ï¼ŒåŸºäºå†…å®¹åˆ†æ)ã€‚"""

        start_time = time_module.time()
        debug_logger.info(
            "å¼€å§‹è·å–ç ”æŠ¥æ•°æ®", symbol=symbol, days=days, analysis_date=analysis_date
        )
        print(
            f"ğŸ“‘ [UnifiedDataAccess] æ­£åœ¨è·å– {symbol} æœºæ„ç ”æŠ¥æ•°æ®ï¼ˆæœ€è¿‘{days}å¤©ï¼ŒåŒ…å«å†…å®¹ï¼‰..."
        )

        data: Dict[str, Any] = {
            "symbol": symbol,
            "research_reports": [],
            "data_success": False,
            "source": None,
            "report_count": 0,
            "analysis_summary": {},
            "content_analysis": {},
        }

        # åªæ”¯æŒ A è‚¡
        if not self._is_chinese_stock(symbol):
            data["error"] = "æœºæ„ç ”æŠ¥æ•°æ®ä»…æ”¯æŒä¸­å›½Aè‚¡è‚¡ç¥¨"
            print("   âš ï¸ æœºæ„ç ”æŠ¥æ•°æ®ä»…æ”¯æŒAè‚¡")
            debug_logger.warning("ç ”æŠ¥æ•°æ®ä»…æ”¯æŒAè‚¡", symbol=symbol)
            return data

        # 1. ä¼˜å…ˆä½¿ç”¨ Tushare report_rc æ¥å£
        if data_source_manager.tushare_available:
            try:
                print("   [æ–¹æ³•1-Tushare] æ­£åœ¨è·å–ç ”æŠ¥æ•°æ®ï¼ˆreport_rcæ¥å£ï¼ŒåŒ…å«å†…å®¹ï¼‰...")
                ts_code = data_source_manager._convert_to_ts_code(symbol)

                # è®¡ç®—æ—¥æœŸèŒƒå›´
                if analysis_date:
                    end_date = analysis_date
                    base_date = datetime.strptime(analysis_date, "%Y%m%d")
                else:
                    end_date = datetime.now().strftime("%Y%m%d")
                    base_date = datetime.now()
                start_date = (base_date - timedelta(days=days)).strftime("%Y%m%d")

                with network_optimizer.apply():
                    df_reports = data_source_manager.tushare_api.report_rc(
                        ts_code=ts_code,
                        start_date=start_date,
                        end_date=end_date,
                    )

                if df_reports is not None and not df_reports.empty:
                    print(f"   âœ“ è·å–åˆ° {len(df_reports)} æ¡Tushareç ”æŠ¥æ•°æ®ï¼ˆå«å†…å®¹ï¼‰")

                    # å»é‡ï¼šåŸºäºæ—¥æœŸ+æœºæ„+æ ‡é¢˜
                    if len(df_reports) > 0:
                        df_reports["_unique_key"] = (
                            df_reports["report_date"].astype(str)
                            + "_"
                            + df_reports["org_name"].astype(str)
                            + "_"
                            + df_reports["report_title"].astype(str)
                        )
                        df_reports = df_reports.drop_duplicates(
                            subset=["_unique_key"], keep="first"
                        )
                        df_reports = df_reports.drop(columns=["_unique_key"])
                        print(
                            f"   âœ“ å»é‡åå‰©ä½™ {len(df_reports)} æ¡ç ”æŠ¥æ•°æ®"
                        )

                    analysis = self._analyze_research_reports(df_reports)

                    # å†æ¬¡åœ¨å­—å…¸å±‚é¢å»é‡
                    seen_keys = set()
                    reports: List[Dict[str, Any]] = []
                    for report_data in analysis.get("reports_data", []):
                        unique_key = (
                            str(report_data.get("report_date", ""))
                            + "_"
                            + str(report_data.get("org_name", ""))
                            + "_"
                            + str(report_data.get("report_title", ""))
                        )
                        if unique_key in seen_keys:
                            continue
                        seen_keys.add(unique_key)

                        reports.append(
                            {
                                "æ—¥æœŸ": report_data.get("report_date", ""),
                                "ç ”æŠ¥æ ‡é¢˜": report_data.get("report_title", ""),
                                "æœºæ„åç§°": report_data.get("org_name", ""),
                                "ç ”ç©¶å‘˜": report_data.get("author_name", ""),
                                "è¯„çº§": report_data.get("rating", ""),
                                "ç›®æ ‡ä»·": str(
                                    report_data.get("target_price_max")
                                    or report_data.get("target_price_min")
                                    or "N/A"
                                ),
                                "ç ”æŠ¥ç±»å‹": report_data.get("report_type", ""),
                                "ç ”æŠ¥å†…å®¹": report_data.get("content", ""),
                                "å†…å®¹æ‘˜è¦": report_data.get(
                                    "content_summary", ""
                                ),
                            }
                        )

                    data["research_reports"] = reports
                    data["report_count"] = analysis.get("total_reports", 0)
                    data["analysis_summary"] = analysis.get("summary", {})
                    data["content_analysis"] = analysis.get(
                        "content_analysis", {}
                    )
                    data["data_success"] = True
                    data["source"] = "tushare"

                    print(
                        f"   âœ… æˆåŠŸè·å– {len(reports)} æ¡æœºæ„ç ”æŠ¥ï¼ˆå«å†…å®¹å’Œå†…å®¹åˆ†æï¼‰"
                    )
                    debug_logger.info(
                        "ç ”æŠ¥æ•°æ®è·å–æˆåŠŸï¼ˆTushareï¼Œå«å†…å®¹ï¼‰",
                        symbol=symbol,
                        count=len(reports),
                        source="tushare",
                    )

                    elapsed_time = time_module.time() - start_time
                    debug_logger.info(
                        "ç ”æŠ¥æ•°æ®è·å–å®Œæˆ",
                        symbol=symbol,
                        success=True,
                        count=len(reports),
                        elapsed=f"{elapsed_time:.2f}s",
                    )
                    return data
                else:
                    print("   â„¹ï¸ Tushareæœªæ‰¾åˆ°ç ”æŠ¥æ•°æ®")
            except Exception as e:  # noqa: BLE001
                debug_logger.warning("Tushareè·å–ç ”æŠ¥å¤±è´¥", error=e, symbol=symbol)
                print(f"   âš ï¸ Tushareè·å–å¤±è´¥: {e}")

        # 2. å¤‡é€‰ä½¿ç”¨ Akshare
        try:
            print("   [æ–¹æ³•2-Akshare] æ­£åœ¨è·å–ç ”æŠ¥æ•°æ®ï¼ˆå¤‡ç”¨æ•°æ®æºï¼‰...")
            with network_optimizer.apply():
                import akshare as ak  # type: ignore

                df = ak.stock_research_report_em(symbol=symbol)

            if df is not None and not df.empty:
                seen_keys = set()
                reports = []
                for _, row in df.iterrows():
                    date = str(row.get("æ—¥æœŸ", ""))
                    org = str(row.get("æœºæ„åç§°", ""))
                    title = str(row.get("ç ”æŠ¥æ ‡é¢˜", ""))
                    unique_key = f"{date}_{org}_{title}"
                    if unique_key in seen_keys:
                        continue
                    seen_keys.add(unique_key)

                    report = {
                        "æ—¥æœŸ": date,
                        "ç ”æŠ¥æ ‡é¢˜": title,
                        "æœºæ„åç§°": org,
                        "ç ”ç©¶å‘˜": str(row.get("ç ”ç©¶å‘˜", "")),
                        "è¯„çº§": str(row.get("è¯„çº§", "")),
                        "ç›®æ ‡ä»·": str(row.get("ç›®æ ‡ä»·", "N/A")),
                        "ç›¸å…³è‚¡ç¥¨": str(row.get("ç›¸å…³è‚¡ç¥¨", "")),
                        "ç ”æŠ¥å†…å®¹": "",
                        "å†…å®¹æ‘˜è¦": "",
                    }
                    reports.append(report)

                rating_list = [r["è¯„çº§"] for r in reports if r["è¯„çº§"]]
                total = len(reports)
                buy_count = sum(
                    1
                    for r in rating_list
                    if any(
                        keyword in str(r)
                        for keyword in ["ä¹°å…¥", "å¢æŒ", "æ¨è", "å¼ºæ¨"]
                    )
                )
                neutral_count = sum(
                    1
                    for r in rating_list
                    if any(
                        keyword in str(r)
                        for keyword in ["æŒæœ‰", "ä¸­æ€§", "è§‚æœ›"]
                    )
                )
                sell_count = sum(
                    1
                    for r in rating_list
                    if any(
                        keyword in str(r)
                        for keyword in ["å–å‡º", "å‡æŒ", "å›é¿"]
                    )
                )

                data["research_reports"] = reports
                data["report_count"] = len(reports)
                data["analysis_summary"] = {
                    "rating_ratio": {
                        "buy_ratio": round(buy_count / total * 100, 2)
                        if total > 0
                        else 0,
                        "neutral_ratio": round(neutral_count / total * 100, 2)
                        if total > 0
                        else 0,
                        "sell_ratio": round(sell_count / total * 100, 2)
                        if total > 0
                        else 0,
                    }
                }
                data["data_success"] = True
                data["source"] = "akshare"

                print(f"   âœ… æˆåŠŸè·å– {len(reports)} æ¡æœºæ„ç ”æŠ¥ï¼ˆAkshareï¼‰")
                debug_logger.info(
                    "ç ”æŠ¥æ•°æ®è·å–æˆåŠŸï¼ˆAkshareï¼‰",
                    symbol=symbol,
                    count=len(reports),
                    source="akshare",
                )
            else:
                print("   â„¹ï¸ æœªæ‰¾åˆ°æœºæ„ç ”æŠ¥æ•°æ®")
                data["error"] = "æœªæ‰¾åˆ°æœºæ„ç ”æŠ¥æ•°æ®"

        except Exception as e:  # noqa: BLE001
            debug_logger.error("è·å–æœºæ„ç ”æŠ¥å¤±è´¥", error=e, symbol=symbol)
            print(f"   âŒ è·å–æœºæ„ç ”æŠ¥å¤±è´¥: {e}")
            data["error"] = str(e)

        elapsed_time = time_module.time() - start_time
        debug_logger.info(
            "ç ”æŠ¥æ•°æ®è·å–å®Œæˆ",
            symbol=symbol,
            success=data.get("data_success", False),
            count=data.get("report_count", 0),
            elapsed=f"{elapsed_time:.2f}s",
        )

        return data

    def get_announcement_data(
        self,
        symbol: str,
        days: int = 30,
        analysis_date: Optional[str] = None,
    ) -> Dict[str, Any]:
        """è·å–å…¬å‘Šæ•°æ® - è¿‡å» N å¤©çš„ä¸Šå¸‚å…¬å¸å…¬å‘Š (ä¸œæ–¹è´¢å¯Œä¼˜å…ˆï¼Œå…¶æ¬¡ Tushare)ã€‚"""

        start_time = time_module.time()
        debug_logger.info(
            "å¼€å§‹è·å–å…¬å‘Šæ•°æ®",
            symbol=symbol,
            days=days,
            analysis_date=analysis_date,
            method="get_announcement_data",
        )
        print(f"ğŸ“¢ [UnifiedDataAccess] æ­£åœ¨è·å– {symbol} æœ€è¿‘{days}å¤©çš„å…¬å‘Šæ•°æ®...")

        data: Dict[str, Any] = {
            "symbol": symbol,
            "announcements": [],
            "pdf_analysis": [],
            "data_success": False,
            "source": None,
            "days": days,
            "date_range": None,
        }

        # åªæ”¯æŒ A è‚¡
        if not self._is_chinese_stock(symbol):
            data["error"] = "å…¬å‘Šæ•°æ®ä»…æ”¯æŒä¸­å›½Aè‚¡è‚¡ç¥¨"
            debug_logger.warning("å…¬å‘Šæ•°æ®ä»…æ”¯æŒAè‚¡", symbol=symbol, is_chinese=False)
            print("   âš ï¸ å…¬å‘Šæ•°æ®ä»…æ”¯æŒAè‚¡")
            return data

        def _normalize_url(url: Optional[str]) -> Optional[str]:
            if not url:
                return None
            url = url.strip()
            if not url:
                return None
            if url.startswith("//"):
                return "https:" + url
            if url.startswith("/"):
                return "https://static.cninfo.com.cn" + url
            return url

        def _resolve_pdf_url(
            row: Dict[str, Any], ts_code_value: str, ann_date_value: str
        ) -> Optional[str]:
            key_priority = [
                "pdf_url",
                "file_url",
                "adjunct_url",
                "page_pdf_url",
                "ann_pdf_url",
                "url",
                "page_url",
                "doc_url",
                "src",
            ]
            for key in key_priority:
                value = row.get(key)
                normalized = (
                    _normalize_url(value) if isinstance(value, str) else None
                )
                if normalized:
                    return normalized

            ann_id = row.get("announcement_id") or row.get("attachment_id")
            org_id = row.get("org_id") or row.get("orgId")
            announcement_type = row.get("announcement_type") or row.get("plate")
            if ann_id and org_id:
                if not announcement_type:
                    if ts_code_value.endswith(".SH"):
                        announcement_type = "sse"
                    elif ts_code_value.endswith(".SZ"):
                        announcement_type = "szse"
                    elif ts_code_value.endswith(".BJ"):
                        announcement_type = "bj"
                return (
                    "https://www.cninfo.com.cn/new/disclosure/detail"
                    f"?plate={announcement_type or ''}&orgId={org_id}"
                    f"&stockCode={ts_code_value.replace('.', '')}"
                    f"&announcementId={ann_id}"
                    + (
                        f"&announcementTime={ann_date_value}" if ann_date_value else ""
                    )
                )

            return None

        def _extract_pdf_text(pdf_bytes: bytes) -> Optional[str]:
            text_candidates: List[str] = []
            # ä¼˜å…ˆå°è¯• PyPDF2
            try:
                import PyPDF2  # type: ignore

                reader = PyPDF2.PdfReader(BytesIO(pdf_bytes))
                page_texts = []
                for page in reader.pages[:20]:
                    extracted = page.extract_text() or ""
                    page_texts.append(extracted.strip())
                combined = "\n".join(filter(None, page_texts)).strip()
                if combined:
                    text_candidates.append(combined)
            except Exception as e:  # noqa: BLE001
                debug_logger.debug("PyPDF2è§£æå…¬å‘ŠPDFå¤±è´¥", error=str(e))

            # å¤‡ç”¨ pdfplumber
            if not text_candidates:
                try:
                    import pdfplumber  # type: ignore

                    with pdfplumber.open(BytesIO(pdf_bytes)) as pdf:
                        page_texts = []
                        for page in pdf.pages[:20]:
                            page_texts.append(page.extract_text() or "")
                        combined = "\n".join(filter(None, page_texts)).strip()
                        if combined:
                            text_candidates.append(combined)
                except Exception as e:  # noqa: BLE001
                    debug_logger.debug(
                        "pdfplumberè§£æå…¬å‘ŠPDFå¤±è´¥", error=str(e)
                    )

            if text_candidates:
                text = text_candidates[0]
                if len(text) > 8000:
                    return text[:8000] + "..."
                return text
            return None

        session = requests.Session()
        session.headers.update({"User-Agent": "Mozilla/5.0"})

        def _em_cookies() -> Dict[str, str]:
            """æ„é€ ä¸œæ–¹è´¢å¯Œ pdf.dfcfw.com åçˆ¬è„šæœ¬è®¾ç½®çš„ Cookieã€‚"""

            status = 208722705 + 1275103711 + 1998477227
            return {
                "__tst_status": f"{status}#",
                "EO_Bot_Ssid": "212402176",
            }

        def _cninfo_download_url(detail_url: str) -> Optional[str]:
            try:
                parsed = urlparse(detail_url)
                qs = parse_qs(parsed.query)
                ann_id = qs.get("announcementId") or qs.get("bulletinId")
                ann_time = qs.get("announcementTime") or qs.get("announceTime")
                if ann_id and ann_time:
                    return (
                        "https://www.cninfo.com.cn/new/announcement/download"
                        f"?bulletinId={ann_id[0]}&announceTime={ann_time[0]}"
                    )
            except Exception:  # noqa: BLE001
                pass
            return None

        def _download_pdf_bytes(
            url: str, origin_detail: Optional[str] = None, depth: int = 0
        ) -> Optional[bytes]:
            if not url or not isinstance(url, str) or depth > 2:
                return None
            try:
                try:
                    parsed = urlparse(url)
                    host = parsed.netloc.lower()
                except Exception:  # noqa: BLE001
                    host = ""

                if "pdf.dfcfw.com" in host:
                    headers_em = {
                        "User-Agent": (
                            "Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:140.0) "
                            "Gecko/20100101 Firefox/140.0"
                        ),
                        "Accept": "application/pdf,*/*;q=0.9",
                        "Accept-Language": "zh-CN,zh;q=0.9",
                    }
                    cookies_em = _em_cookies()
                    response = requests.get(
                        url,
                        headers=headers_em,
                        cookies=cookies_em,
                        timeout=20,
                        allow_redirects=True,
                        proxies={},
                    )
                else:
                    headers = {"User-Agent": "Mozilla/5.0"}
                    if origin_detail and depth == 0:
                        headers["Referer"] = origin_detail
                        with network_optimizer.apply():
                            session.get(
                                origin_detail,
                                headers=headers,
                                timeout=25,
                                allow_redirects=True,
                            )
                    cninfo_download = _cninfo_download_url(url)
                    request_url = cninfo_download or url
                    if origin_detail:
                        headers["Referer"] = origin_detail
                    with network_optimizer.apply():
                        response = session.get(
                            request_url,
                            headers=headers,
                            timeout=25,
                            allow_redirects=True,
                        )
                if response.status_code != 200:
                    debug_logger.debug(
                        "å…¬å‘ŠPDFä¸‹è½½å¤±è´¥", url=url, status=response.status_code
                    )
                    return None

                content = response.content
                content_type = response.headers.get("Content-Type", "").lower()
                if content.startswith(b"%PDF") or "application/pdf" in content_type:
                    return content
                if content.startswith(b"PK"):
                    try:
                        with zipfile.ZipFile(BytesIO(content)) as zf:
                            for name in zf.namelist():
                                if name.lower().endswith(".pdf"):
                                    return zf.read(name)
                    except Exception as zip_error:  # noqa: BLE001
                        debug_logger.debug(
                            "å…¬å‘ŠPDFè§£å‹å¤±è´¥", url=url, error=str(zip_error)
                        )

                text_snippet = content[:1024].decode("utf-8", errors="ignore")
                if "<html" in text_snippet.lower():
                    html_text = response.text
                    pdf_match = re.search(
                        r"https?://static\\.cninfo\\.com\\.cn/[^\\\"'<>]+\\.pdf",
                        html_text,
                        re.I,
                    )
                    if pdf_match:
                        next_url = pdf_match.group(0)
                        debug_logger.debug(
                            "å…¬å‘ŠPDFé“¾æ¥é‡å®šå‘",
                            original=url,
                            extracted=next_url,
                        )
                        return _download_pdf_bytes(
                            next_url, origin_detail or url, depth + 1
                        )

                    ann_id_match = re.search(
                        r"announcementId=([A-Za-z0-9]+)", url
                    )
                    org_id_match = re.search(r"orgId=([A-Za-z0-9]+)", url)
                    if ann_id_match and org_id_match:
                        ann_id = ann_id_match.group(1)
                        org_id = org_id_match.group(1)
                        api_url = (
                            "https://www.cninfo.com.cn/new/disclosure/detail"
                            f"?plate=&orgId={org_id}&stockCode=&announcementId={ann_id}&lang=zh"
                        )
                        with network_optimizer.apply():
                            api_resp = requests.get(
                                api_url,
                                headers=headers,
                                timeout=25,
                                allow_redirects=True,
                            )
                        if api_resp.status_code == 200:
                            api_text = api_resp.text
                            pdf_match_api = re.search(
                                r"https?://static\\.cninfo\\.com\\.cn/[^\\\"'<>]+\\.pdf",
                                api_text,
                                re.I,
                            )
                            if pdf_match_api:
                                next_url = pdf_match_api.group(0)
                                debug_logger.debug(
                                    "å…¬å‘ŠPDFé“¾æ¥(AJAX)é‡å®šå‘",
                                    original=url,
                                    extracted=next_url,
                                )
                                return _download_pdf_bytes(
                                    next_url, origin_detail or url, depth + 1
                                )
                    pdf_match_rel = re.search(
                        r"data-pdf=\"([^\"]+\.pdf)\"", html_text
                    )
                    if pdf_match_rel:
                        next_url = _normalize_url(pdf_match_rel.group(1))
                        if next_url:
                            debug_logger.debug(
                                "å…¬å‘ŠPDFé“¾æ¥é‡å®šå‘(data-pdf)",
                                original=url,
                                extracted=next_url,
                            )
                            return _download_pdf_bytes(
                                next_url, origin_detail or url, depth + 1
                            )
                    href_match = re.search(
                        r'href="([^\"]+\.pdf)"', html_text
                    )
                    if href_match:
                        next_url = _normalize_url(href_match.group(1))
                        if next_url:
                            debug_logger.debug(
                                "å…¬å‘ŠPDFé“¾æ¥é‡å®šå‘(href)",
                                original=url,
                                extracted=next_url,
                            )
                            return _download_pdf_bytes(
                                next_url, origin_detail or url, depth + 1
                            )
                return None
            except Exception as e:  # noqa: BLE001
                debug_logger.debug("å…¬å‘ŠPDFä¸‹è½½å¼‚å¸¸", url=url, error=str(e))
                return None

        def _download_and_parse_pdf(
            url: str, ann_meta: Optional[Dict[str, Any]] = None
        ) -> Tuple[Optional[str], Optional[str]]:
            detail_url = None
            if ann_meta:
                detail_url = (
                    ann_meta.get("detail_url")
                    if ann_meta.get("detail_url") != "N/A"
                    else None
                )
            pdf_bytes = _download_pdf_bytes(url, detail_url)
            if not pdf_bytes:
                return None, None
            text = _extract_pdf_text(pdf_bytes)

            saved_path = None
            if pdf_bytes:
                title = ann_meta.get("å…¬å‘Šæ ‡é¢˜") if ann_meta else "announcement"
                trade_date = (
                    ann_meta.get("æ—¥æœŸ")
                    if ann_meta
                    else datetime.now().strftime("%Y-%m-%d")
                )
                safe_title = re.sub(r"[\\/:*?\"<>|]", "_", str(title))
                safe_date = re.sub(r"[\\/:*?\"<>|]", "_", str(trade_date))
                symbol_dir = Path("data") / "announcements" / symbol
                symbol_dir.mkdir(parents=True, exist_ok=True)
                filename = f"{safe_date}_{safe_title}.pdf"
                saved_path = str(symbol_dir / filename)
                with open(saved_path, "wb") as f:
                    f.write(pdf_bytes)

            return text, saved_path

        def _fetch_announcements_from_eastmoney(
            symbol: str,
        ) -> Tuple[List[Dict[str, Any]], List[Dict[str, Any]]]:
            """ä½¿ç”¨ä¸œæ–¹è´¢å¯Œå…¬å‘Šæ¥å£ä½œä¸ºå…œåº•æ•°æ®æºï¼Œè¿”å›å…¬å‘Šåˆ—è¡¨å’Œ PDF è§£æç»“æœã€‚"""

            base_url = "https://np-anotice-stock.eastmoney.com/api/security/ann"
            content_api = "https://np-cnotice-stock.eastmoney.com/api/content/ann"

            def _clean_symbol(code: str) -> str:
                stock = code.strip()
                if "." in stock:
                    stock = stock.split(".")[0]
                for prefix in ("sh", "sz", "gb_", "us", "us_"):
                    if stock.startswith(prefix):
                        stock = stock[len(prefix) :]
                        break
                return stock

            headers_list = {
                "Host": "np-anotice-stock.eastmoney.com",
                "Referer": "https://data.eastmoney.com/notices/hsa/5.html",
                "User-Agent": (
                    "Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:140.0) "
                    "Gecko/20100101 Firefox/140.0"
                ),
            }

            def _get_notices(
                code: str, page_size: int = 50, page_index: int = 1
            ) -> List[Dict[str, Any]]:
                params = {
                    "page_size": page_size,
                    "page_index": page_index,
                    "ann_type": "SHA,CYB,SZA,BJA,INV",
                    "client_source": "web",
                    "f_node": "0",
                    "stock_list": _clean_symbol(code),
                }
                resp = requests.get(
                    base_url, params=params, headers=headers_list, timeout=15
                )
                resp.raise_for_status()
                payload = resp.json() or {}
                return payload.get("data", {}).get("list", []) or []

            def _fetch_notice_detail(art_code: str) -> Dict[str, Any]:
                if not art_code:
                    return {}
                params = {"art_code": art_code, "client_source": "web"}
                headers_detail = {
                    "Referer": "https://data.eastmoney.com/",
                    "User-Agent": headers_list["User-Agent"],
                    "Accept": "application/json,text/plain,*/*",
                }
                try:
                    resp = requests.get(
                        content_api,
                        params=params,
                        headers=headers_detail,
                        timeout=15,
                        proxies={},
                    )
                    if resp.status_code != 200:
                        return {}
                    return resp.json().get("data", {}) or {}
                except Exception as e:  # noqa: BLE001
                    try:
                        debug_logger.debug(
                            "ä¸œæ–¹è´¢å¯Œå…¬å‘Šè¯¦æƒ…è¯·æ±‚å¤±è´¥",
                            art_code=art_code,
                            error=str(e),
                        )
                    except Exception:  # noqa: BLE001
                        pass
                    return {}

            def _extract_pdf_urls(detail: Dict[str, Any]) -> List[str]:
                urls: List[str] = []
                if not detail:
                    return urls
                attaches = (
                    detail.get("attachments")
                    or detail.get("attach_list")
                    or []
                )
                for att in attaches:
                    if not isinstance(att, Dict):
                        continue
                    u = (
                        att.get("url")
                        or att.get("oss_url")
                        or att.get("file_url")
                    )
                    if isinstance(u, str) and u.lower().endswith(".pdf"):
                        urls.append(u)
                for key in ("pdf_url", "em_pdf", "notice_pdf"):
                    u = detail.get(key)
                    if isinstance(u, str) and u.lower().endswith(".pdf"):
                        urls.append(u)
                seen: Dict[str, None] = {}
                result_urls: List[str] = []
                for u in urls:
                    if u not in seen:
                        seen[u] = None
                        result_urls.append(u)
                return result_urls

            notices = _get_notices(symbol, page_size=50)
            if not notices:
                return [], []

            notices.sort(key=lambda x: x.get("notice_date", ""), reverse=True)
            max_items = 20
            notices = notices[:max_items]

            announcements: List[Dict[str, Any]] = []
            pdf_analysis: List[Dict[str, Any]] = []

            for notice in notices:
                stock_info = (notice.get("codes") or [{}])[0] or {}
                art_code = notice.get("art_code") or ""
                date_str = notice.get("notice_date") or ""
                title = notice.get("title") or "N/A"
                ann_type = (notice.get("columns") or [{}])[0].get(
                    "column_name", ""
                )

                ann_item: Dict[str, Any] = {
                    "æ—¥æœŸ": date_str or "N/A",
                    "å…¬å‘Šæ ‡é¢˜": title,
                    "å…¬å‘Šç±»å‹": ann_type or "N/A",
                    "å…¬å‘Šæ‘˜è¦": "",
                    "pdf_url": (
                        f"https://pdf.dfcfw.com/pdf/H2_{art_code}_1.pdf"
                        if art_code
                        else "N/A"
                    ),
                    "download_url": (
                        f"https://pdf.dfcfw.com/pdf/H2_{art_code}_1.pdf"
                        if art_code
                        else "N/A"
                    ),
                    "detail_url": (
                        f"https://data.eastmoney.com/notices/detail/{stock_info.get('stock_code', '')}/{art_code}.html"
                        if art_code
                        else "N/A"
                    ),
                    "åŸå§‹æ•°æ®": notice,
                }

                detail = _fetch_notice_detail(art_code)
                pdf_urls = _extract_pdf_urls(detail)
                if pdf_urls:
                    ann_item["pdf_url"] = pdf_urls[0]
                    ann_item["download_url"] = pdf_urls[0]

                announcements.append(ann_item)

            for ann in announcements[:5]:
                pdf_url = ann.get("pdf_url")
                analysis_entry: Dict[str, Any] = {
                    "date": ann.get("æ—¥æœŸ"),
                    "title": ann.get("å…¬å‘Šæ ‡é¢˜"),
                    "pdf_url": pdf_url,
                    "text": None,
                    "success": False,
                }
                if pdf_url and pdf_url != "N/A":
                    pdf_text, saved_path = _download_and_parse_pdf(pdf_url, ann)
                    if pdf_text:
                        analysis_entry["text"] = pdf_text
                        analysis_entry["success"] = True
                    if saved_path:
                        analysis_entry["saved_path"] = saved_path
                        ann["saved_path"] = saved_path
                else:
                    analysis_entry["text"] = "æœªæä¾›PDFé“¾æ¥ã€‚"
                pdf_analysis.append(analysis_entry)

            return announcements, pdf_analysis

        try:
            if analysis_date:
                end_dt = datetime.strptime(analysis_date, "%Y%m%d")
            else:
                end_dt = datetime.now()

            start_dt = end_dt - timedelta(days=days)
            start_date_str = start_dt.strftime("%Y%m%d")
            end_date_str = end_dt.strftime("%Y%m%d")
            data["date_range"] = {"start": start_date_str, "end": end_date_str}

            # 1) é¦–é€‰ä¸œæ–¹è´¢å¯Œå…¬å‘Šæ¥å£
            print("   [Eastmoney] æ­£åœ¨é€šè¿‡ä¸œæ–¹è´¢å¯Œå…¬å‘Šæ¥å£è·å–æ•°æ®...")
            anns_em, pdf_em = _fetch_announcements_from_eastmoney(symbol)
            if anns_em:
                data["announcements"] = anns_em
                data["pdf_analysis"] = pdf_em
                data["source"] = "eastmoney"
                data["data_success"] = True
                data["count"] = len(anns_em)
                return data

            # 2) ä¸œæ–¹è´¢å¯Œæ— æ•°æ®æ—¶ï¼Œå°è¯• Tushare anns_d
            if not data_source_manager.tushare_available:
                data["error"] = "ä¸œæ–¹è´¢å¯Œå…¬å‘Šæ¥å£æ— æ•°æ®ä¸”Tushareä¸å¯ç”¨"
                print("   âš ï¸ ä¸œæ–¹è´¢å¯Œå…¬å‘Šæ¥å£æ— æ•°æ®ï¼Œä¸”å½“å‰ç¯å¢ƒæœªå¯ç”¨Tushare")
                return data

            ts_code = data_source_manager._convert_to_ts_code(symbol)

            print("   [Tushare] ä¸œæ–¹è´¢å¯Œæ— æ•°æ®ï¼Œå°è¯•é€šè¿‡Tushare anns_d è·å–å…¬å‘Šåˆ—è¡¨...")
            all_rows: List[pd.DataFrame] = []
            limit = 50
            offset = 0
            while True:
                with network_optimizer.apply():
                    df_batch = data_source_manager.tushare_api.anns_d(
                        ts_code=ts_code,
                        start_date=start_date_str,
                        end_date=end_date_str,
                        limit=limit,
                        offset=offset,
                        fields=(
                            "ts_code,ann_date,ann_type,title,content,file_url,adjunct_url,"
                            "page_pdf_url,pdf_url,org_id,announcement_id,announcement_type,src,url"
                        ),
                    )

                if df_batch is None or df_batch.empty:
                    break

                all_rows.append(df_batch)
                if len(df_batch) < limit:
                    break
                offset += limit

            if not all_rows:
                print("   â„¹ï¸ ä¸œæ–¹è´¢å¯Œä¸Tushareå‡æœªæŸ¥è¯¢åˆ°å…¬å‘Šæ•°æ®")
                data["error"] = "ä¸œæ–¹è´¢å¯Œä¸Tushareå‡æœªæŸ¥è¯¢åˆ°å…¬å‘Šæ•°æ®"
                return data

            df_all = pd.concat(all_rows, ignore_index=True)
            df_all = df_all.sort_values("ann_date", ascending=False)

            announcements_ts: List[Dict[str, Any]] = []
            for _, row in df_all.iterrows():
                ann_date = str(row.get("ann_date", ""))
                ann_date_fmt = "N/A"
                if ann_date:
                    try:
                        ann_date_fmt = datetime.strptime(
                            ann_date, "%Y%m%d"
                        ).strftime("%Y-%m-%d")
                    except Exception:  # noqa: BLE001
                        ann_date_fmt = ann_date

                pdf_url = _resolve_pdf_url(row, ts_code, ann_date)
                download_url = (
                    _cninfo_download_url(pdf_url) if pdf_url else None
                )
                announcement = {
                    "æ—¥æœŸ": ann_date_fmt,
                    "å…¬å‘Šæ ‡é¢˜": str(row.get("title", "N/A")),
                    "å…¬å‘Šç±»å‹": str(row.get("ann_type", "N/A")),
                    "å…¬å‘Šæ‘˜è¦": (
                        str(row.get("content", ""))[:400]
                        if pd.notna(row.get("content"))
                        else ""
                    ),
                    "pdf_url": download_url or pdf_url or "N/A",
                    "download_url": download_url or pdf_url or "N/A",
                    "detail_url": pdf_url or "N/A",
                    "åŸå§‹æ•°æ®": {k: row.get(k) for k in row.index},
                }
                announcements_ts.append(announcement)

            if not announcements_ts:
                print("   â„¹ï¸ Tushare å…¬å‘Šæ•°æ®ä¸ºç©º")
                data["error"] = "ä¸œæ–¹è´¢å¯Œå…¬å‘Šæ¥å£æ— æ•°æ®ä¸”Tushareå…¬å‘Šæ•°æ®ä¸ºç©º"
                return data

            data["announcements"] = announcements_ts
            data["source"] = "tushare"
            data["data_success"] = True
            data["count"] = len(announcements_ts)

            pdf_analysis_ts: List[Dict[str, Any]] = []
            for ann in announcements_ts[:5]:
                pdf_url = ann.get("pdf_url")
                analysis_entry_ts: Dict[str, Any] = {
                    "date": ann.get("æ—¥æœŸ"),
                    "title": ann.get("å…¬å‘Šæ ‡é¢˜"),
                    "pdf_url": pdf_url,
                    "text": None,
                    "success": False,
                }
                if pdf_url and pdf_url != "N/A":
                    pdf_text, saved_path = _download_and_parse_pdf(pdf_url, ann)
                    if pdf_text:
                        analysis_entry_ts["text"] = pdf_text
                        analysis_entry_ts["success"] = True
                    if saved_path:
                        analysis_entry_ts["saved_path"] = saved_path
                        ann["saved_path"] = saved_path
                else:
                    analysis_entry_ts["text"] = "æœªæä¾›PDFé“¾æ¥ã€‚"
                pdf_analysis_ts.append(analysis_entry_ts)

            data["pdf_analysis"] = pdf_analysis_ts

        except Exception as e:  # noqa: BLE001
            debug_logger.error("è·å–å…¬å‘Šæ•°æ®å¤±è´¥", error=str(e), symbol=symbol)
            data["error"] = str(e)

        elapsed_time = time_module.time() - start_time
        debug_logger.info(
            "å…¬å‘Šæ•°æ®è·å–å®Œæˆ",
            symbol=symbol,
            success=data.get("data_success", False),
            count=data.get("count", 0),
            elapsed=f"{elapsed_time:.2f}s",
        )

        return data

    def get_chip_distribution_data(
        self,
        symbol: str,
        trade_date: str | None = None,
        current_price: float | None = None,
        analysis_date: Optional[str] = None,
    ) -> Dict[str, Any]:
        """è·å–ç­¹ç åˆ†å¸ƒæ•°æ® - ä½¿ç”¨ Tushare çš„ cyq_perf å’Œ cyq_chips æ¥å£ï¼ˆä»… A è‚¡ï¼‰ã€‚

        è¡Œä¸ºä¸æ ¹ç›®å½• unified_data_access.UnifiedDataAccess.get_chip_distribution_data ä¿æŒä¸€è‡´ï¼š
        - è¿”å› cyq_perf / cyq_chips åŸå§‹æ•°æ®ï¼›
        - ç”Ÿæˆ summary æ±‡æ€»å­—æ®µï¼›
        - è°ƒç”¨ _analyze_chip_changes ç”Ÿæˆ 30 å¤©ç­¹ç å˜åŒ–åˆ†æã€‚
        """

        start_time = time_module.time()
        if analysis_date and not trade_date:
            trade_date = analysis_date

        debug_logger.info(
            "å¼€å§‹è·å–ç­¹ç åˆ†å¸ƒæ•°æ®",
            symbol=symbol,
            trade_date=trade_date,
            analysis_date=analysis_date,
            method="get_chip_distribution_data",
        )
        print(f"ğŸ¯ [UnifiedDataAccess] æ­£åœ¨è·å– {symbol} çš„ç­¹ç åˆ†å¸ƒæ•°æ®...")

        data: Dict[str, Any] = {
            "symbol": symbol,
            "data_success": False,
            "cyq_perf": None,
            "cyq_chips": None,
            "latest_date": None,
            "source": None,
        }

        # åªæ”¯æŒ A è‚¡
        if not self._is_chinese_stock(symbol):
            data["error"] = "ç­¹ç åˆ†å¸ƒæ•°æ®ä»…æ”¯æŒä¸­å›½Aè‚¡è‚¡ç¥¨"
            debug_logger.warning("ç­¹ç æ•°æ®ä»…æ”¯æŒAè‚¡", symbol=symbol, is_chinese=False)
            print("   âš ï¸ ç­¹ç åˆ†å¸ƒæ•°æ®ä»…æ”¯æŒAè‚¡")
            return data

        try:
            if not data_source_manager.tushare_available:
                data["error"] = "Tushareæ•°æ®æºä¸å¯ç”¨ï¼Œç­¹ç åˆ†å¸ƒæ•°æ®éœ€è¦Tushareæ”¯æŒ"
                print("   âš ï¸ Tushareä¸å¯ç”¨ï¼Œæ— æ³•è·å–ç­¹ç åˆ†å¸ƒæ•°æ®")
                return data

            print("   [Tushare] æ­£åœ¨è·å–ç­¹ç åˆ†å¸ƒæ•°æ®...")
            ts_code = data_source_manager._convert_to_ts_code(symbol)

            if not trade_date:
                trade_date = datetime.now().strftime("%Y%m%d")

            # æ–¹æ³•1: cyq_perf - è¿‡å»30å¤©ç­¹ç åŠèƒœç‡æ•°æ®
            try:
                print("   [æ–¹æ³•1] æ­£åœ¨è·å–cyq_perfæ•°æ®ï¼ˆç­¹ç åˆ†å¸ƒåŠèƒœç‡ï¼‰...")
                end_date = trade_date
                start_date = (
                    datetime.strptime(end_date, "%Y%m%d")
                    - timedelta(days=30)
                ).strftime("%Y%m%d")

                with network_optimizer.apply():
                    df_perf = data_source_manager.tushare_api.cyq_perf(
                        ts_code=ts_code,
                        start_date=start_date,
                        end_date=end_date,
                    )

                if (
                    df_perf is not None
                    and isinstance(df_perf, pd.DataFrame)
                    and not df_perf.empty
                ):
                    perf_records = df_perf.to_dict("records")
                    latest_perf = perf_records[-1] if perf_records else None

                    data["cyq_perf"] = {
                        "data": perf_records,
                        "latest": latest_perf,
                        "count": len(perf_records),
                    }

                    if latest_perf:
                        data["latest_date"] = latest_perf.get(
                            "trade_date", trade_date
                        )

                    print(
                        f"   [æ–¹æ³•1] âœ… æˆåŠŸè·å– {len(perf_records)} æ¡cyq_perfæ•°æ®"
                    )
                    debug_logger.info(
                        "Tushare cyq_perfè·å–æˆåŠŸ",
                        symbol=symbol,
                        count=len(perf_records),
                        latest_date=data.get("latest_date"),
                    )
                else:
                    print("   [æ–¹æ³•1] âš ï¸ æœªè·å–åˆ°cyq_perfæ•°æ®")
            except Exception as e:  # noqa: BLE001
                debug_logger.warning(
                    "Tushare cyq_perfè·å–å¤±è´¥", error=str(e), symbol=symbol
                )
                print(f"   [æ–¹æ³•1] âŒ å¤±è´¥: {e}")

            # æ–¹æ³•2: cyq_chips - æŒ‡å®šæ—¥æœŸåŠå›æº¯æ•°æ—¥çš„ç­¹ç åˆ†å¸ƒ
            try:
                print("   [æ–¹æ³•2] æ­£åœ¨è·å–cyq_chipsæ•°æ®ï¼ˆæ¯æ—¥ç­¹ç åˆ†å¸ƒï¼‰...")
                with network_optimizer.apply():
                    df_chips = data_source_manager.tushare_api.cyq_chips(
                        ts_code=ts_code, trade_date=trade_date
                    )

                if (
                    df_chips is not None
                    and isinstance(df_chips, pd.DataFrame)
                    and not df_chips.empty
                ):
                    chips_records = df_chips.to_dict("records")
                    data["cyq_chips"] = {
                        "data": chips_records,
                        "count": len(chips_records),
                        "trade_date": trade_date,
                    }

                    if not data.get("latest_date"):
                        data["latest_date"] = trade_date

                    print(
                        f"   [æ–¹æ³•2] âœ… æˆåŠŸè·å– {len(chips_records)} æ¡cyq_chipsæ•°æ®"
                    )
                    debug_logger.info(
                        "Tushare cyq_chipsè·å–æˆåŠŸ",
                        symbol=symbol,
                        count=len(chips_records),
                        trade_date=trade_date,
                    )
                else:
                    print(
                        f"   [æ–¹æ³•2] âš ï¸ {trade_date}æœªè·å–åˆ°æ•°æ®ï¼Œå°è¯•è·å–æœ€è¿‘äº¤æ˜“æ—¥æ•°æ®..."
                    )
                    for i in range(1, 6):
                        try_date = (
                            datetime.strptime(trade_date, "%Y%m%d")
                            - timedelta(days=i)
                        ).strftime("%Y%m%d")
                        with network_optimizer.apply():
                            df_chips = data_source_manager.tushare_api.cyq_chips(
                                ts_code=ts_code,
                                trade_date=try_date,
                            )
                        if (
                            df_chips is not None
                            and isinstance(df_chips, pd.DataFrame)
                            and not df_chips.empty
                        ):
                            chips_records = df_chips.to_dict("records")
                            data["cyq_chips"] = {
                                "data": chips_records,
                                "count": len(chips_records),
                                "trade_date": try_date,
                            }
                            data["latest_date"] = try_date
                            print(
                                f"   [æ–¹æ³•2] âœ… æˆåŠŸè·å– {try_date} çš„ {len(chips_records)} æ¡cyq_chipsæ•°æ®"
                            )
                            break
                    else:
                        print(
                            "   [æ–¹æ³•2] âš ï¸ æœ€è¿‘5ä¸ªè‡ªç„¶æ—¥å‡æœªè·å–åˆ°cyq_chipsæ•°æ®"
                        )
            except Exception as e:  # noqa: BLE001
                debug_logger.warning(
                    "Tushare cyq_chipsè·å–å¤±è´¥", error=str(e), symbol=symbol
                )
                print(f"   [æ–¹æ³•2] âŒ å¤±è´¥: {e}")

            # æˆåŠŸæ€§åˆ¤æ–­ä¸æ±‡æ€»
            if data.get("cyq_perf") or data.get("cyq_chips"):
                data["data_success"] = True
                data["source"] = "tushare"

                summary: Dict[str, Any] = {}
                latest = None
                if data.get("cyq_perf") and data["cyq_perf"].get("latest"):
                    latest = data["cyq_perf"]["latest"]
                    summary["äº¤æ˜“æ—¥æœŸ"] = latest.get("trade_date", "N/A")
                    summary["5%æˆæœ¬"] = latest.get("cost_5pct", "N/A")
                    summary["15%æˆæœ¬"] = latest.get("cost_15pct", "N/A")
                    summary["50%æˆæœ¬ï¼ˆä¸­ä½ï¼‰"] = latest.get("cost_50pct", "N/A")
                    summary["85%æˆæœ¬"] = latest.get("cost_85pct", "N/A")
                    summary["95%æˆæœ¬"] = latest.get("cost_95pct", "N/A")
                    summary["åŠ æƒå¹³å‡æˆæœ¬"] = latest.get("weight_avg", "N/A")
                    summary["å†å²æœ€ä½"] = latest.get("his_low", "N/A")
                    summary["å†å²æœ€é«˜"] = latest.get("his_high", "N/A")

                    if (
                        pd.notna(latest.get("cost_50pct"))
                        and pd.notna(latest.get("cost_85pct"))
                        and pd.notna(latest.get("cost_15pct"))
                    ):
                        try:
                            cost_range = float(latest["cost_85pct"]) - float(
                                latest["cost_15pct"]
                            )
                            cost_center = float(latest["cost_50pct"])
                            if cost_center > 0:
                                concentration_pct = (
                                    cost_range / cost_center
                                ) * 100
                                if concentration_pct < 10:
                                    summary["ç­¹ç é›†ä¸­åº¦"] = "é«˜"
                                elif concentration_pct > 30:
                                    summary["ç­¹ç é›†ä¸­åº¦"] = "ä½"
                                else:
                                    summary["ç­¹ç é›†ä¸­åº¦"] = "ä¸­ç­‰"
                                summary["æˆæœ¬åŒºé—´"] = f"{cost_range:.2f} ({concentration_pct:.1f}%)"
                        except Exception:  # noqa: BLE001
                            summary["ç­¹ç é›†ä¸­åº¦"] = "N/A"
                    else:
                        summary["ç­¹ç é›†ä¸­åº¦"] = "N/A"

                    summary["æ•°æ®æœŸæ•°"] = data["cyq_perf"].get("count", 0)

                if (
                    data.get("cyq_perf")
                    and data["cyq_perf"].get("data")
                    and len(data["cyq_perf"]["data"]) >= 2
                ):
                    analysis_price = current_price
                    if (
                        (not analysis_price)
                        and latest
                        and pd.notna(latest.get("weight_avg"))
                    ):
                        try:
                            analysis_price = float(latest.get("weight_avg", 0))
                        except Exception:  # noqa: BLE001
                            analysis_price = None

                    change_analysis = self._analyze_chip_changes(
                        data["cyq_perf"]["data"], analysis_price
                    )
                    if change_analysis:
                        summary["30å¤©å˜åŒ–åˆ†æ"] = change_analysis
                        data["change_analysis"] = change_analysis

                if data.get("cyq_chips"):
                    summary["ç­¹ç åˆ†å¸ƒæ•°æ®ç‚¹"] = data["cyq_chips"].get("count")
                    summary["ç­¹ç åˆ†å¸ƒæ—¥æœŸ"] = data["cyq_chips"].get(
                        "trade_date", "N/A"
                    )

                data["summary"] = summary

                print(
                    f"   âœ… ç­¹ç åˆ†å¸ƒæ•°æ®è·å–å®Œæˆï¼ˆæ•°æ®æ—¥æœŸ: {data.get('latest_date', 'N/A')}ï¼‰"
                )
                debug_logger.info(
                    "ç­¹ç åˆ†å¸ƒæ•°æ®è·å–æˆåŠŸ",
                    symbol=symbol,
                    has_perf=data.get("cyq_perf") is not None,
                    has_chips=data.get("cyq_chips") is not None,
                    latest_date=data.get("latest_date"),
                )
            else:
                data["error"] = "æœªèƒ½è·å–ç­¹ç åˆ†å¸ƒæ•°æ®ï¼Œcyq_perfå’Œcyq_chipså‡å¤±è´¥"
                print("   âš ï¸ æ‰€æœ‰æ•°æ®æºå‡æœªè·å–åˆ°ç­¹ç æ•°æ®")

        except Exception as e:  # noqa: BLE001
            debug_logger.error("è·å–ç­¹ç æ•°æ®å¤±è´¥", error=str(e), symbol=symbol)
            print(f"   âŒ è·å–ç­¹ç æ•°æ®å¤±è´¥: {e}")
            try:
                import traceback

                traceback.print_exc()
            except Exception:  # noqa: BLE001
                pass
            data["error"] = str(e)

        elapsed_time = time_module.time() - start_time
        debug_logger.info(
            "ç­¹ç æ•°æ®è·å–å®Œæˆ",
            symbol=symbol,
            success=data.get("data_success", False),
            source=data.get("source"),
            has_perf=data.get("cyq_perf") is not None,
            has_chips=data.get("cyq_chips") is not None,
            elapsed=f"{elapsed_time:.2f}s",
        )

        return data

    def _analyze_research_reports(self, df_reports: pd.DataFrame) -> Dict[str, Any]:
        """åˆ†æç ”æŠ¥æ•°æ®ï¼ˆå¢å¼ºç‰ˆï¼‰ã€‚"""

        if df_reports is None or df_reports.empty:
            return {
                "total_reports": 0,
                "reports_data": [],
                "summary": {},
            }

        analysis: Dict[str, Any] = {
            "total_reports": len(df_reports),
            "reports_data": [],
            "summary": {},
        }

        all_contents: List[str] = []

        if len(df_reports) > 0:
            debug_logger.debug(
                f"report_rcæ¥å£è¿”å›çš„åˆ—å: {df_reports.columns.tolist()}"
            )

        for _, row in df_reports.iterrows():
            content = ""  # Tushare report_rc å½“å‰ä¸æä¾›å®Œæ•´å†…å®¹

            content_summary = ""
            if content:
                if len(content) > 500:
                    content_summary = content[:500] + "..."
                else:
                    content_summary = content
                all_contents.append(content)

            report_data = {
                "report_date": str(row.get("report_date", "")),
                "report_title": str(row.get("report_title", "")),
                "org_name": str(row.get("org_name", "")),
                "author_name": str(row.get("author_name", "")),
                "rating": str(row.get("rating", "")),
                "report_type": str(row.get("report_type", "")),
                "classify": str(row.get("classify", "")),
                "quarter": str(row.get("quarter", "")),
                "target_price_max": row.get("max_price"),
                "target_price_min": row.get("min_price"),
                "op_rt": row.get("op_rt"),
                "op_pr": row.get("op_pr"),
                "np": row.get("np"),
                "eps": row.get("eps"),
                "pe": row.get("pe"),
                "roe": row.get("roe"),
                "ev_ebitda": row.get("ev_ebitda"),
                "content": content,
                "content_summary": content_summary,
            }
            analysis["reports_data"].append(report_data)

        if all_contents:
            analysis["content_analysis"] = self._analyze_research_content(
                all_contents
            )

        if len(df_reports) > 0:
            if "org_name" in df_reports.columns:
                org_counts = df_reports["org_name"].value_counts()
                analysis["summary"]["top_institutions"] = (
                    org_counts.head(5).to_dict()
                )

            if "rating" in df_reports.columns:
                rating_counts = df_reports["rating"].value_counts()
                analysis["summary"]["rating_distribution"] = (
                    rating_counts.to_dict()
                )

                total = len(df_reports)
                buy_count = sum(
                    1
                    for r in rating_counts.index
                    if any(
                        keyword in str(r)
                        for keyword in ["ä¹°å…¥", "å¢æŒ", "æ¨è", "å¼ºæ¨"]
                    )
                )
                neutral_count = sum(
                    1
                    for r in rating_counts.index
                    if any(
                        keyword in str(r)
                        for keyword in ["æŒæœ‰", "ä¸­æ€§", "è§‚æœ›"]
                    )
                )
                sell_count = sum(
                    1
                    for r in rating_counts.index
                    if any(
                        keyword in str(r)
                        for keyword in ["å–å‡º", "å‡æŒ", "å›é¿"]
                    )
                )

                analysis["summary"]["rating_ratio"] = {
                    "buy_ratio": round(buy_count / total * 100, 2)
                    if total > 0
                    else 0,
                    "neutral_ratio": round(neutral_count / total * 100, 2)
                    if total > 0
                    else 0,
                    "sell_ratio": round(sell_count / total * 100, 2)
                    if total > 0
                    else 0,
                }

            if "max_price" in df_reports.columns:
                max_prices = df_reports["max_price"].dropna()
                if not max_prices.empty:
                    analysis["summary"]["target_price_stats"] = {
                        "max": float(max_prices.max()),
                        "min": float(max_prices.min()),
                        "avg": float(max_prices.mean()),
                        "count": len(max_prices),
                    }
            elif "min_price" in df_reports.columns:
                min_prices = df_reports["min_price"].dropna()
                if not min_prices.empty:
                    analysis["summary"]["target_price_stats"] = {
                        "max": float(min_prices.max()),
                        "min": float(min_prices.min()),
                        "avg": float(min_prices.mean()),
                        "count": len(min_prices),
                    }

            for col in ["eps", "pe", "roe"]:
                if col in df_reports.columns:
                    values = df_reports[col].dropna()
                    if not values.empty:
                        analysis["summary"][f"{col}_stats"] = {
                            "max": float(values.max()),
                            "min": float(values.min()),
                            "avg": float(values.mean()),
                        }

            if len(df_reports) > 0:
                latest_report = df_reports.iloc[0]
                analysis["summary"]["latest_report"] = {
                    "date": str(latest_report.get("report_date", "")),
                    "title": str(latest_report.get("report_title", "")),
                    "org": str(latest_report.get("org_name", "")),
                    "rating": str(latest_report.get("rating", "")),
                    "target_price": latest_report.get("max_price")
                    or latest_report.get("min_price"),
                }

        if "content_analysis" not in analysis:
            analysis["content_analysis"] = {}

        return analysis

    def _analyze_research_content(self, contents: List[str]) -> Dict[str, Any]:
        """åˆ†æç ”æŠ¥å†…å®¹ã€‚"""

        if not contents:
            return {
                "has_content": False,
                "total_length": 0,
                "avg_length": 0,
                "key_topics": [],
                "sentiment_analysis": {},
            }

        combined_content = " ".join([c for c in contents if c])
        total_length = len(combined_content)
        avg_length = total_length / len(contents) if contents else 0

        key_topics: List[str] = []
        common_keywords = [
            "å¢é•¿",
            "ä¸šç»©",
            "ç›ˆåˆ©",
            "æ”¶å…¥",
            "å‡€åˆ©æ¶¦",
            "EPS",
            "ROE",
            "ä¼°å€¼",
            "ä¹°å…¥",
            "æŒæœ‰",
            "æ¨è",
            "ç›®æ ‡ä»·",
            "é£é™©",
            "æœºä¼š",
            "å‰æ™¯",
            "è¡Œä¸š",
            "å¸‚åœº",
            "ç«äº‰",
            "ä¼˜åŠ¿",
            "åˆ›æ–°",
            "è½¬å‹",
            "æ‰©å¼ ",
        ]

        content_lower = combined_content.lower()
        for keyword in common_keywords:
            if keyword in content_lower:
                key_topics.append(keyword)

        positive_words = [
            "å¢é•¿",
            "æå‡",
            "æ”¹å–„",
            "åˆ©å¥½",
            "çœ‹å¥½",
            "ä¹°å…¥",
            "æ¨è",
            "æœºä¼š",
            "ä¼˜åŠ¿",
        ]
        negative_words = [
            "ä¸‹é™",
            "ä¸‹æ»‘",
            "é£é™©",
            "æ‹…å¿§",
            "å–å‡º",
            "å‡æŒ",
            "æŒ‘æˆ˜",
            "å›°éš¾",
        ]

        positive_count = sum(1 for word in positive_words if word in content_lower)
        negative_count = sum(1 for word in negative_words if word in content_lower)

        sentiment = "neutral"
        if positive_count > negative_count * 1.5:
            sentiment = "positive"
        elif negative_count > positive_count * 1.5:
            sentiment = "negative"

        return {
            "has_content": True,
            "total_reports_with_content": len([c for c in contents if c]),
            "total_length": total_length,
            "avg_length": round(avg_length, 0),
            "key_topics": key_topics[:10],
            "sentiment_analysis": {
                "sentiment": sentiment,
                "positive_signals": positive_count,
                "negative_signals": negative_count,
                "sentiment_score": round(
                    (positive_count - negative_count)
                    / max(positive_count + negative_count, 1)
                    * 100,
                    2,
                ),
            },
        }

    def _analyze_chip_changes(
        self, perf_data: list, current_price: float | None = None
    ) -> Optional[Dict[str, Any]]:
        """åˆ†æè¿‡å»30å¤©ç­¹ç åˆ†å¸ƒå˜åŒ–ï¼Œåˆ¤æ–­ä¸»åŠ›èµ„é‡‘è¡Œä¸ºã€‚

        ç›´æ¥è¿ç§»è‡ªæ—§ unified_data_access._analyze_chip_changesï¼Œä¿æŒåˆ†æå£å¾„ä¸€è‡´ã€‚
        """

        if not perf_data or len(perf_data) < 2:
            return None

        try:
            sorted_data = sorted(
                perf_data,
                key=lambda x: str(x.get("trade_date", "")),
                reverse=False,
            )
            earliest = sorted_data[0]
            latest = sorted_data[-1]

            analysis: Dict[str, Any] = {
                "period": f"{earliest.get('trade_date', 'N/A')} è‡³ {latest.get('trade_date', 'N/A')}",
                "days_count": len(sorted_data),
                "cost_changes": {},
                "concentration_changes": {},
                "main_force_behavior": {},
                "chip_peak_analysis": {},
            }

            # 1. æˆæœ¬ä»·æ ¼å˜åŒ–
            cost_fields = [
                "cost_5pct",
                "cost_15pct",
                "cost_50pct",
                "cost_85pct",
                "cost_95pct",
                "weight_avg",
            ]
            for field in cost_fields:
                earliest_val = earliest.get(field)
                latest_val = latest.get(field)
                if pd.notna(earliest_val) and pd.notna(latest_val):
                    try:
                        change = float(latest_val) - float(earliest_val)
                        change_pct = (
                            (change / float(earliest_val)) * 100
                            if float(earliest_val) > 0
                            else 0
                        )
                        analysis["cost_changes"][field] = {
                            "earliest": round(float(earliest_val), 2),
                            "latest": round(float(latest_val), 2),
                            "change": round(change, 2),
                            "change_pct": round(change_pct, 2),
                        }
                    except Exception:  # noqa: BLE001
                        pass

            # 2. ç­¹ç é›†ä¸­åº¦å˜åŒ–
            def calc_concentration(record: Dict[str, Any]):
                try:
                    cost_15 = float(record.get("cost_15pct", 0))
                    cost_85 = float(record.get("cost_85pct", 0))
                    cost_50 = float(record.get("cost_50pct", 0))
                    if cost_50 > 0:
                        range_pct = ((cost_85 - cost_15) / cost_50) * 100
                        if range_pct < 10:
                            return "é«˜", range_pct
                        if range_pct > 30:
                            return "ä½", range_pct
                        return "ä¸­", range_pct
                except Exception:  # noqa: BLE001
                    pass
                return None, None

            earliest_conc_level, earliest_conc_pct = calc_concentration(earliest)
            latest_conc_level, latest_conc_pct = calc_concentration(latest)

            if earliest_conc_level and latest_conc_level:
                analysis["concentration_changes"] = {
                    "earliest_level": earliest_conc_level,
                    "latest_level": latest_conc_level,
                    "earliest_pct": round(earliest_conc_pct, 2)
                    if earliest_conc_pct
                    else None,
                    "latest_pct": round(latest_conc_pct, 2)
                    if latest_conc_pct
                    else None,
                    "trend": (
                        "æå‡"
                        if latest_conc_pct < earliest_conc_pct
                        else "ä¸‹é™"
                        if latest_conc_pct > earliest_conc_pct
                        else "ç¨³å®š"
                    ),
                }

            # 3. ç­¹ç å³°ç§»åŠ¨åˆ†æ
            if (
                "cost_changes" in analysis
                and "weight_avg" in analysis["cost_changes"]
            ):
                weight_avg_change = analysis["cost_changes"]["weight_avg"][
                    "change"
                ]
                cost_50_change = analysis["cost_changes"].get("cost_50pct", {}).get(
                    "change", 0
                )

                if weight_avg_change > 0 and cost_50_change > 0:
                    analysis["chip_peak_analysis"]["peak_direction"] = "ä¸Šç§»"
                    analysis["chip_peak_analysis"]["peak_speed"] = (
                        "å¿«é€Ÿ"
                        if abs(weight_avg_change) > abs(cost_50_change) * 1.5
                        else "ç¼“æ…¢"
                    )
                elif weight_avg_change < 0 and cost_50_change < 0:
                    analysis["chip_peak_analysis"]["peak_direction"] = "ä¸‹ç§»"
                    analysis["chip_peak_analysis"]["peak_speed"] = (
                        "å¿«é€Ÿ"
                        if abs(weight_avg_change) > abs(cost_50_change) * 1.5
                        else "ç¼“æ…¢"
                    )
                else:
                    analysis["chip_peak_analysis"]["peak_direction"] = "éœ‡è¡"
                    analysis["chip_peak_analysis"]["peak_speed"] = "ä¸ç¨³å®š"

            # 4. ä¸»åŠ›èµ„é‡‘è¡Œä¸ºåˆ¤æ–­
            main_force_signals: list[str] = []
            behavior_score = 0

            if analysis["concentration_changes"].get("trend") == "æå‡":
                if latest_conc_level in ["é«˜", "ä¸­"]:
                    main_force_signals.append("é›†ä¸­åº¦æå‡ï¼Œå¯èƒ½ä¸»åŠ›æ”¶é›†ç­¹ç ")
                    behavior_score += 2

            if "weight_avg" in analysis["cost_changes"]:
                weight_change = analysis["cost_changes"]["weight_avg"]["change"]
                if weight_change < 0 and current_price:
                    try:
                        price_vs_cost = (
                            (
                                float(current_price)
                                - float(latest.get("weight_avg", 0))
                            )
                            / float(latest.get("weight_avg", 0))
                            * 100
                        )
                        if price_vs_cost < 10:
                            main_force_signals.append(
                                "å¹³å‡æˆæœ¬ä¸‹é™ä¸”è‚¡ä»·æ¥è¿‘æˆæœ¬ï¼Œå¯èƒ½ä½ä½å¸ç­¹"
                            )
                            behavior_score += 2
                    except Exception:  # noqa: BLE001
                        pass

            if analysis["chip_peak_analysis"].get("peak_direction") == "ä¸Šç§»":
                if (
                    "cost_85pct" in analysis["cost_changes"]
                    and "cost_15pct" in analysis["cost_changes"]
                ):
                    high_cost_increase = analysis["cost_changes"]["cost_85pct"][
                        "change"
                    ]
                    low_cost_change = analysis["cost_changes"]["cost_15pct"][
                        "change"
                    ]
                    if (
                        high_cost_increase > 0
                        and abs(high_cost_increase)
                        > abs(low_cost_change) * 1.5
                    ):
                        main_force_signals.append(
                            "é«˜ä½æˆæœ¬å¿«é€Ÿä¸Šå‡ï¼Œç­¹ç å³°ä¸Šç§»ï¼Œå¯èƒ½è·åˆ©å‡ºé€ƒ"
                        )
                        behavior_score -= 3

            if analysis["concentration_changes"].get("trend") == "ä¸‹é™":
                if latest_conc_level == "ä½":
                    main_force_signals.append(
                        "é›†ä¸­åº¦ä¸‹é™ä¸”åŒºé—´æ‰©å¤§ï¼Œå¯èƒ½æ•£æˆ·æ¥ç›˜"
                    )
                    behavior_score -= 2

            if (
                "cost_5pct" in analysis["cost_changes"]
                and "cost_50pct" in analysis["cost_changes"]
            ):
                low_stable = abs(
                    analysis["cost_changes"]["cost_5pct"]["change"]
                ) < abs(
                    analysis["cost_changes"]["cost_5pct"]["earliest"]
                ) * 0.1
                mid_up = analysis["cost_changes"]["cost_50pct"]["change"] > 0
                if low_stable and mid_up:
                    main_force_signals.append(
                        "ä½ä½æˆæœ¬ç¨³å®šï¼Œä¸­ä½æˆæœ¬ä¸Šç§»ï¼Œå¯èƒ½æ´—ç›˜åæ‹‰å‡"
                    )
                    behavior_score += 1

            if behavior_score >= 3:
                main_force_judgment = "æ”¶é›†ä½ä»·ç­¹ç "
                main_force_confidence = "é«˜"
            elif behavior_score >= 1:
                main_force_judgment = "å¯èƒ½æ”¶é›†ç­¹ç "
                main_force_confidence = "ä¸­"
            elif behavior_score <= -3:
                main_force_judgment = "è·åˆ©å‡ºé€ƒ"
                main_force_confidence = "é«˜"
            elif behavior_score <= -1:
                main_force_judgment = "å¯èƒ½è·åˆ©äº†ç»“"
                main_force_confidence = "ä¸­"
            else:
                main_force_judgment = "éœ‡è¡æ•´ç†"
                main_force_confidence = "ä½"

            analysis["main_force_behavior"] = {
                "judgment": main_force_judgment,
                "confidence": main_force_confidence,
                "score": behavior_score,
                "signals": main_force_signals,
                "description": self._generate_main_force_description(
                    main_force_judgment, main_force_signals, analysis
                ),
            }

            return analysis

        except Exception as e:  # noqa: BLE001
            debug_logger.warning("ç­¹ç å˜åŒ–åˆ†æå¤±è´¥", error=str(e))
            try:
                import traceback

                traceback.print_exc()
            except Exception:  # noqa: BLE001
                pass
            return None

    def _generate_main_force_description(
        self, judgment: str, signals: list, analysis: Dict[str, Any]
    ) -> str:
        """ç”Ÿæˆä¸»åŠ›è¡Œä¸ºæè¿°æ–‡æœ¬ï¼ˆè¿ç§»è‡ªæ—§ç‰ˆå®ç°ï¼‰ã€‚"""

        desc = f"ä¸»åŠ›è¡Œä¸ºåˆ¤æ–­: {judgment}\n"
        desc += (
            f"ç½®ä¿¡åº¦: {analysis.get('main_force_behavior', {}).get('confidence', 'N/A')}\n\n"
        )

        if signals:
            desc += "å…³é”®ä¿¡å·:\n"
            for i, signal in enumerate(signals, 1):
                desc += f"{i}. {signal}\n"

        peak = analysis.get("chip_peak_analysis", {})
        desc += (
            f"\nç­¹ç å³°å˜åŒ–: {peak.get('peak_direction', 'N/A')} "
            f"({peak.get('peak_speed', 'N/A')})\n"
        )

        if "cost_changes" in analysis and "weight_avg" in analysis["cost_changes"]:
            change_info = analysis["cost_changes"]["weight_avg"]
            desc += (
                f"å¹³å‡æˆæœ¬å˜åŒ–: {change_info['earliest']:.2f} â†’ {change_info['latest']:.2f} "
                f"({change_info['change']:+.2f}, {change_info['change_pct']:+.2f}%)\n"
            )

        conc = analysis.get("concentration_changes", {})
        if conc:
            desc += (
                f"ç­¹ç é›†ä¸­åº¦å˜åŒ–: {conc.get('earliest_level', 'N/A')} â†’ {conc.get('latest_level', 'N/A')} "
                f"({conc.get('trend', 'N/A')})"
            )

        return desc

    # ------------------------------------------------------------------
    # å·¥å…·æ–¹æ³•
    # ------------------------------------------------------------------

    def _get_appropriate_trade_date(
        self, analysis_date: Optional[str] = None
    ) -> str:
        """é€‰æ‹©åˆé€‚çš„äº¤æ˜“æ—¥ï¼Œç”¨äº Tushare æ—¥çº¿/ä¼°å€¼æŸ¥è¯¢ã€‚

        è¿™æ˜¯åŸå§‹å®ç°ä¸­çš„ä¸€ä¸ªç®€åŒ–ç‰ˆæœ¬ï¼š
        - è‹¥ç»™å®š analysis_dateï¼Œåˆ™ç›´æ¥è¿”å›ï¼›
        - å¦åˆ™ä½¿ç”¨å½“å‰æ—¥æœŸï¼ˆè‹¥éäº¤æ˜“æ—¥ï¼Œå›é€€åˆ°æœ€è¿‘çš„äº¤æ˜“æ—¥ï¼‰ã€‚
        """

        if analysis_date:
            return analysis_date

        # æ— æ˜¾å¼åˆ†ææ—¥æœŸæ—¶ï¼Œå°è¯•ä½¿ç”¨æœ€è¿‘çš„ä¸€ä¸ªäº¤æ˜“æ—¥
        try:
            if not data_source_manager.tushare_available:
                return datetime.now().strftime("%Y%m%d")

            today = datetime.now().strftime("%Y%m%d")
            with network_optimizer.apply():
                cal = data_source_manager.tushare_api.trade_cal(
                    start_date=(datetime.now() - timedelta(days=10)).strftime(
                        "%Y%m%d"
                    ),
                    end_date=today,
                    is_open=1,
                )
            if cal is None or cal.empty:
                return today
            trade_dates = cal["cal_date"].tolist()
            return str(trade_dates[-1])
        except Exception:  # noqa: BLE001
            return datetime.now().strftime("%Y%m%d")

    def get_beta_coefficient(self, symbol: str) -> Optional[float]:
        """å ä½å®ç°ï¼šBeta è®¡ç®—é€»è¾‘æ²¿ç”¨åŸå§‹æ•°æ®æºå®ç°ã€‚

        å½“å‰ç‰ˆæœ¬ç›´æ¥å§”æ‰˜ç»™ data_source_managerï¼ˆè‹¥å…¶æä¾›è¯¥èƒ½åŠ›ï¼‰ï¼Œ
        å¦åˆ™è¿”å› Noneã€‚
        """

        try:
            if hasattr(data_source_manager, "get_beta_coefficient"):
                return data_source_manager.get_beta_coefficient(symbol)
        except Exception as e:  # noqa: BLE001
            debug_logger.debug("get_beta_coefficientå¤±è´¥", error=str(e), symbol=symbol)
        return None

    def get_52week_high_low(self, symbol: str) -> Optional[Dict[str, Any]]:
        """è·å– 52 å‘¨é«˜ä½ä½ï¼ˆå¦‚æœæ•°æ®æºæ”¯æŒï¼‰ã€‚"""

        try:
            if hasattr(data_source_manager, "get_52week_high_low"):
                return data_source_manager.get_52week_high_low(symbol)
        except Exception as e:  # noqa: BLE001
            debug_logger.debug("get_52week_high_lowå¤±è´¥", error=str(e), symbol=symbol)
        return None

    def _is_chinese_stock(self, symbol: str) -> bool:
        """åˆ¤æ–­æ˜¯å¦ä¸ºä¸­å›½ A è‚¡ï¼ˆåŸºäº 6 ä½æ•°å­—ä»£ç çš„ç®€å•è§„åˆ™ï¼‰ã€‚"""

        return symbol.isdigit() and len(symbol) == 6
